---
icon: '8'
---

# 프로그래밍 언어 처리 – 코드가 컴퓨터에서 실행되기까지의 여정

이번 장에서 프로그래밍 언어가 어떻게 구현되는지를 살펴봅니다. 기계어라는 실행 가능한 형태로 어떻게 변환되는지를 배우게 돼요\~

우리 인간은 고수준 언어(Python, Java 등)로 프로그램을 씁니다. 하지만 CPU는 기계어만 이해하죠. 그렇다면 질문이 생깁니다.

> "내가 짠 코드는 CPU가 어떻게 이해하는 걸까?"

이 질문에 답하기 위해, **프로그래밍 언어 처리**의 긴 여정을 함께 따라가봅시다.

프로그래밍 언어의 계층은 사진과 같습니다!

<figure><img src="../../.gitbook/assets/image (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 1. 기계어는 너무 괴로워! 어셈블리 언어의 등장

&#x20;기계어와 일대일 대응이 되는 컴퓨터 프로그래밍의 저급 언어입니다.



컴퓨터 초창기, 프로그래머들은 **모든 명령어를 직접 이진수로 작성**해야 했습니다. 예를 들어, 피보나치 수를 계산하려면 `1010001000001010` 같은 기계어 명령어를 하나하나 입력해야 했죠. 너무 고통스러운 일이었기 때문에, 사람들은 **좀 더 인간적인 방법**을 고민하게 됩니다.

#### &#x20;그래서 등장한 것이 바로 **어셈블리 언어**입니다.

{% hint style="danger" %}
기계어의고통을 줄이기 위해, 인간 친화적인 **니모닉(mnemonic)**&#xC744; 사용하는 어셈블리 언어가 등장했습니다.
{% endhint %}

니모닉(mnemonic) : `LOAD`, `STORE`, `ADD` 같은 읽기 쉬운 단어로 기계어를 표현합니다.

```
load #0
store first
```

➡ 이것이 기계어로 번역되는 작업은 어셈블러가 담당합니다.

* 주소에는 `심볼`이라는 이름을 붙여 가독성을 높이고,  `의사명령어(Pseudo-instruction)`로 **메모리 할당**도 명시할 수 있죠.
* **레이블(label)** : 명령어 주소에 의미 있는 이름을 붙여줍니다.
* **주석(comment)** : 코드에 설명을 달 수 있어 다른 사람이 이해하기 쉬워집니다.



예를 들어, 아래는 피보나치 수열을 어셈블리 언어로 작성한 예입니다.

```asm
load #0       ; first를 0으로 설정
store first
load #1       ; second를 1로 설정
store second

again: load first
add second
store next

load second
store first
load next
store second
cmp #200
ble again

first: bss 1
second: bss 1
next: bss 1
```

### 보충 정리 : **bssblock 의사명령어 동작 방식 정리**

`bss`는 **변수를 위한 공간만 확보**하고 초기화는 하지 않는 의사 명령어입니다. 어셈블러는 이 코드로부터 **실제 기계어를 생성**합니다.

{% hint style="danger" %}
『한 권으로 읽는 컴퓨터 구조와 프로그래밍』 중, "bssblock started by symbol" 설명:\
“메모리 덩어리(여기서는 1워드)를 예약하되 메모리 안에는 아무 값도 넣지 않는다. 의사명령어는 기계어와 직접 대응되지는 않지만, 어셈블러에게 지시를 내린다는 무슨 뜻일까 자세히 알아보자!
{% endhint %}

**1) 어떤 역할인가요?**

* 이 명령어는 변수의 저장 공간(워드 단위)를 미리 **예약**만 합니다.
* 하지만 해당 공간 안에는 **값을 채우지 않습니다.**

> 쉽게 말해, "여기 이만큼 공간 필요해요!"라고 운영체제에게 말해두는 작업이지만, 거기에 어떤 값을 넣지는 않는 것이죠.

**2) 왜 필요한가요?**

* 프로그램이 실행되기 전까지, <mark style="color:red;">해당 공간에 어떤 값이 들어갈지는 아직 모르기 때문입니다.</mark>
* 예: 피보나치 수열을 저장할 `first`, `second`, `next` 변수들이 있을 때, 이들은 나중에 값이 결정되므로 **초기화하지 않고 공간만 마련해 둡니다.**

```asm
first:   bss    1    ; first가 저장될 위치 (1워드 예약)
second:  bss    1    ; second가 저장될 위치
next:    bss    1    ; next가 저장될 위치
```

**3) 실제로는 어떻게 처리될까요?**

* 어셈블러는 이 명령을 만나면 **기계어 명령어로 변환하지는 않습니다.**
* 대신 **메모리 주소를 조정하여 해당 심볼에 할당할 위치를 예약**합니다.
* 예를 들어 `first`라는 심볼이 시작하는 곳부터 1워드, 그다음에 `second`, `next`가 차례로 이어지는 식으로 할당됩니다.

#### 🔍 비유로 설명해볼게요

> 마치 이삿짐센터에 전화를 걸어 “내일 우리 짐 들어갈 박스 3개만 미리 갖다 주세요. 아직 짐은 안 쌌어요!”라고 말하는 것과 같습니다.

* `bss`는 박스를 주문한 것(공간 확보)
* 값은 아직 안 들어간 짐 (초기화 없음)
* 어셈블러는 이 정보를 모아서 최종 실행파일을 구성합니다.

#### 4)  메모리 구조 속 데이터 세그먼트 차이점 정리

> &#x20;메모리 구조에서 위의 bss를 볼 수 있는데 세그먼트에서 알아보자

**메모리 안에는 다양한 '방'이 있다**

프로그램이 실행될 때 운영체제는 프로그램을 메모리에 적재합니다. 이때 메모리는 다양한 "세그먼트(segment)"라는 방으로 나뉘어 관리되는데, 특히 중요한 세 가지는 다음과 같습니다.

* **`.text` 세그먼트** : 코드(함수, 명령어 등)가 들어감
* **`.data` 세그먼트** : **초기화된 전역 변수**가 들어감
* **`.bss` 세그먼트** : **초기화되지 않은 전역 변수**가 들어감

#### 📦 비유로 이해하는 세그먼트 구조

| 세그먼트    | 실제 역할             | 비유                            |
| ------- | ----------------- | ----------------------------- |
| `.text` | 기계어 명령어 저장 공간     | 레시피 책                         |
| `.data` | 초기값이 정해진 변수 저장 공간 | **이미 포장된 짐** (예: "양말 상자")     |
| `.bss`  | 초기값이 없는 변수 공간만 예약 | **빈 상자만 미리 배치** (나중에 짐 넣을 예정) |

#### 그림으로 보는 세그먼트 구조

<figure><img src="../../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>

```
┌──────────────┐
│ Stack        │ ← 함수 호출 시 사용 (지역 변수 등)
├──────────────┤
│ Heap         │ ← 동적 할당 영역 (malloc 등)
├──────────────┤
│ .bss         │ ← 초기화되지 않은 전역 변수
├──────────────┤
│ .data        │ ← 초기화된 전역 변수
├──────────────┤
│ .text        │ ← 기계어 코드 (함수, 명령 등)
└──────────────┘
```

> 🔎 `.bss`와 `.data`는 **전역 변수, 정적 변수**를 저장한다는 공통점이 있지만, **초기화 여부**에 따라 위치가 다릅니다!

#### 🧪 예시 코드로 비교해보기

```c
int a = 5;  // .data 세그먼트
int b;      // .bss 세그먼트

int main() {
    int c = 10; // 스택 (지역 변수)
    return 0;
}
```

| 변수  | 저장 위치   | 초기화 여부   |
| --- | ------- | -------- |
| `a` | `.data` | ✅ 초기화됨   |
| `b` | `.bss`  | ❌ 초기화 안됨 |
| `c` | Stack   | 지역 변수    |

🧾 차이 정리 표

| 세그먼트    | 저장 대상                      | 초기화 여부         | 크기 포함 여부 | 실행 시 위치      |
| ------- | -------------------------- | -------------- | -------- | ------------ |
| `.text` | 함수 코드                      | 해당 없음          | 예        | 맨 아래(고정)     |
| `.data` | 초기화된 전역/정적 변수              | ✅              | 예        | `.text` 위    |
| `.bss`  | 초기화되지 않은 전역/정적 변수          | ❌ (자동 0으로 초기화) | 예        | `.data` 위    |
| Stack   | 지역 변수, 함수 호출 정보            | ❓ 실행 시 동적 할당   | X        | 가장 위에서 아래로   |
| Heap    | `malloc`, `new` 등으로 할당된 변수 | ❓ 실행 시 동적 할당   | X        | Stack 아래서 위로 |

> `.bss`는 값은 없지만 자리만 잡아둔 공간, `.data`는 값까지 챙겨넣은 변수 저장소입니다.

**🔍 int a와 bss의 유사도? 조금 더 구체하게 정리해보면...**

* **어셈블리의 `bssblock`은 "전역 변수이지만 아직 값이 없을 때"** 사용하는 메모리 예약 방식입니다.
* Java의 전역 변수(예: 클래스 필드)는 선언만 해도 **자동으로 0으로 초기화**되므로 `.bss`와 매우 흡사합니다.
* 하지만 Java의 **지역 변수**는 초기화를 반드시 해야 사용 가능하죠. (`int a;`만 하고 사용하면 에러!)

**💬 결론**

* **Java에서 `int a;`라고만 선언한 필드**는 어셈블리에서 `first: bss 1`처럼 **공간만 잡고, 나중에 값이 들어갈 변수**와 개념적으로 유사합니다.
* 다만 차이점은, **Java는 자동으로 초기화를 수행**(0 할당)하는 반면,\
  어셈블리의 `.bss`는 **값 없이 공간만 잡아놓고**, 실행 시 운영체제가 0으로 초기화합니다.

***

컴퓨터 구조에 따라 사용하는 기계어가 달라지며, 따라서 **기계어에 대응되어 만들어지는 어셈블리어도 각각 다르게 됩니다.**&#x20;

컴퓨터 CPU마다 지원하는 오퍼레이션의 타입과 개수는 제각각이며, 레지스터의 크기와 개수, 저장된 데이터 형의 표현도 각기 다릅니다. 모든 범용 컴퓨터는 기본적으로 동일한 기능을 수행하지만, 기능을 어떤 과정을 거쳐 수행할지는 다를 수 있으며, 이런 차이는 어셈블리어에 반영되게 돼요.

> 그래서 개발자들은 어려운 방식, 즉 직접 손으로 모든 비트를 알아내는 방법으로 최초의 어셈블러를 작성해야 했습니다.&#x20;



## 2. 어셈블리어도 너무 장황해! 고수준 언어의 탄생

어셈블리 언어도 여전히 사람이 작성하기엔 복잡한 면이 있었습니다. 그래서 더 높은 수준의 추상화를 제공하는 `고수준 언어(high-level language)`가 등장합니다.

#### 대표적인 고수준 언어:

* **포트란(FORTRAN)**: 수식을 계산할 수 있는 최초의 고수준 언어
* **베이직(BASIC)**: 초보자를 위한 명령어 기반 언어
* **C 언어**: 구조적 프로그래밍의 대표, 이후 많은 언어의 기반이 됨

```fortran
I = 0
J = 1
5 K = I + J
I = J
J = K
IF (J .LT. 200) GOTO 5
```

GOTO가 많아서 **스파게티 코드**가 되기 쉬웠고, 구조적인 코드 작성을 위해 **구조적 프로그래밍(structured programming)**&#xC774; 대두됩니다.



## 3. 구조적 프로그래밍이란?

### 1) 구조적 프로그래밍의 핵심 개념

1. **순차(Sequence)**
   * 명령은 위에서 아래로, 한 줄씩 차례로 실행됩니다.
2. **분기(Selection)**
   * if, switch 등을 통해 조건에 따라 흐름을 분기합니다.
3. **반복(Iteration)**
   * while, for 같은 반복문을 통해 일정 조건 동안 반복합니다.

> 이 세 가지 요소로 **모든 프로그램의 논리 구조를 만들 수 있다**는 것이 구조적 프로그래밍의 핵심입니다.

### 2) 구조적 언어 vs 비구조적 언어

특징 구조적 언어 비구조적 언어

| 코드 흐름 | 명확한 순서, 조건, 반복          | GOTO로 아무 데나 이동    |
| ----- | ----------------------- | ----------------- |
| 유지보수  | 쉬움 (읽기 쉬움)              | 어려움 (스파게티 코드 발생)  |
| 예시 언어 | C, Pascal, Java, Python | 초기 Fortran, Basic |

### 3) 스파게티 코드란?

> **GOTO 문을 남용하면 코드가 여기저기 점프하게 되어**\
> 마치 **스파게티처럼 복잡하고 꼬인 코드**가 됩니다.

```
10 PRINT "HELLO"
   GOTO 10
```

위처럼 레이블(10)을 붙이고 GOTO로 점프하면, 프로그램 흐름이 제어하기 어려워집니다.\
→ 오류 수정, 로직 파악이 매우 힘들어지죠.

### 4) 구조적 언어의 대표 예시

언어 특징

| **C**            | 실용적이며 구조적 프로그래밍의 대표 주자 |
| ---------------- | ---------------------- |
| **Pascal**       | 교육용 언어로 GOTO 금지, 구조 강조 |
| **Python, Java** | 현대 언어 대부분 구조적 프로그래밍 기반 |
| **JavaScript**   | 예외는 있지만 대체로 구조적 흐름을 따름 |

### 5) 요약

* 구조적 언어는 순차 + 조건 + 반복으로 프로그램을 구성하는 언어예요.
* GOTO 없이 흐름이 명확하게 보여서 **이해와 유지보수가 쉬워요**.
* 현대 언어는 거의 다 구조적입니다. (C, Python, Java, JS, Go 등등)

> 💬 **"구조가 있어야 사람이 이해할 수 있다."**\
> 이것이 구조적 프로그래밍의 시작점이자 이유입니다!

## 4. 어휘 분석과 정규식: 문장을 '토막내는' 기술&#x20;

### 1) 컴파일이란?&#x20;

**주어진 언어로 작성된 컴퓨터 프로그램**을 **다른 언어의 동등한 프로그램으로 변환**하는 프로세스입니다.

* 보통 `high-level 프로그래밍 언어`를 실행 프로그램으로 만들기 위한 `lower level언어(어셈블리어,기계어)`로 바꾸는데 사용합니다.
* 원래의 문서(high-level)를 **소스 코드** 혹은 원시 코드라고 부름
* 출력된 문서(low-level)를 **목적 코드**라고 부름

> 출처 : [https://velog.io/@dbstjd0924/%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC-1.Lexical-Analysis](https://velog.io/@dbstjd0924/%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC-1.Lexical-Analysis)

### 2) Compiler의 조건 <a href="#compiler" id="compiler"></a>

1. 옮김의 과정에서 **프로그램의 뜻이 보존**되어야 함 즉, 입력받은 <mark style="color:red;">프로그램의 의미를 충실히 따라야 합니다.</mark>
2. 입력으로 들어온 프로그램을 실용적으로 개선해야 함

### 3) 컴파일 과정 <a href="#undefined" id="undefined"></a>

![](https://velog.velcdn.com/images/dbstjd0924/post/0f7e30a0-0301-4581-9278-01fc37cb0f63/image.png)

### 4) 어휘 분석

{% hint style="warning" %}
컴파일러의 첫 번째 단계는 소스 코드를 정규 문법 (regular grammar)에 따라 토큰 (token)으로 분류하는 어휘 분석 또는 스캐닝 (scanning)입니다.
{% endhint %}

> 💡 예를 들어, “Hello world"라는 문장에서 'H', 'e', 'l', 'l', 'o'을 따로 놓으면 어떠한 의미도 없지만, "Hello"라는 하나의 조각으로 보면 의미를 갖게 됩니다.

이제 컴퓨터가 프로그래밍 언어를 어떻게 **분해해서 이해하는지** 살펴보겠습니다. 언어를 어떤 식으로 처리하는지 살펴봅시다. 먼저, 어휘 분석을 알아보는 거로!

<figure><img src="../../.gitbook/assets/image (2) (1) (1).png" alt=""><figcaption></figcaption></figure>

컴파일러의 첫 번째 단계는 **소스 코드를 정규 문법에 따라 토큰으로 분류하는 어휘 분석** 또는 스캔입니다.

#### 1단계: 어휘 분석(Lexical Analysis)

* **문자 → 토큰(token)**: 예를 들어 `lex luthor` → `["lex", " ", "luthor"]`
* 토큰은 이름, 숫자, 연산자 등 다양한 타입을 가짐

정규식을 사용하여 부동소수점 숫자를 인식하는 예:

```regex
[+-]?(([0-9]*\.?[0-9]+)|([0-9]+\.?[0-9]*))([Ee][+-]?[0-9]+)?
```

이 정규식은 `1.23`, `-0.5`, `2e10` 등 다양한 형태의 수를 인식할 수 있습니다.



다른예를 들어, "Lexical analysis is the first step of compiler"이라는 문장에서 'L', 'e', 'x', 'i', 'c', 'a', 'l'을 따로 놓으면 어떠한 의미도 없지만, "Lexical"이라는 하나의 조각으로 보면 의미를 갖게 됩니다.

어휘 분석 단계에서 검출되는 의미 있는 조각을 어휘항목이라고 하며, 어휘 분석기는 소스 코드에서 어휘항목을 검출해서 토큰을 생성합니다.

> 여기서 용어를 정의해봅시다.

* 어휘 항목 : 소스 코드에 존재하는 의미있는 문자열, 식별자, 숫자, 키워드 등을 의미합니다.
* 패턴 : 토큰이 어휘항목을 서술하는 규칙으로써 정규문법에 따라 표현됩니다.
* 토큰 : 토큰이름과 속성값으로 구성되는 데이터쌍으로써, 각 토큰은 토큰의 패턴을 부합하는 어휘항목을 갖습니다.
* Lexer (Scanner) :  어휘 분석기를 뜻하며, 토큰을 추출하는 역할

| 구분             | 정의                                                                                 | 예시                          |
| -------------- | ---------------------------------------------------------------------------------- | --------------------------- |
| 어휘 항목( lexeme) | 소스코드에 있는 실제 단어                                                                     | x, 42, int                  |
| 패턴(pattern)    | <ul><li>어휘 항목을 서술하는 규칙</li><li>token이 lexeme를 서술하는 규칙으로써, 정규문법에 따라 표현 됨</li></ul>  | \[a-zA-Z\_]\[a-zA-Z0-9\_]\* |
| 토큰(toekn)      | <ul><li>토큰 이름 + 어휘 항목</li><li>각 token은 token의 pattern에 부합하는 lexeme를 갖는다.</li></ul> | (IDENTIFIER, "x")           |



<figure><img src="../../.gitbook/assets/image (275).png" alt=""><figcaption></figcaption></figure>

lexeme는 token의 예시라고 생각하면 되고, token은 카테고리라고 생각하면 된다. 그리고 **lexeme와 token을 잇는 규칙**이 `pattern`입니다.&#x20;

* `pattern = lexeme(index,indentifier)`

아래의 그림의 코드가 있다고 생각해봅시다.

![](https://velog.velcdn.com/images/dbstjd0924/post/774abb5f-e6c2-475c-90b9-a42491d07b82/image.png)

소스 코드상으로는 저렇게 보이는 코드지만,

> `\tif(i==j)\n\t\tz=0;\n\teles\n\t\tz=1;` 이런 식으로 이루어져 있어요.

여기서 우리가 해야할 일은 특정한 문법으로 나누어줘야 하는 것입니다.\
\
이런 패턴이 존재한다면 위의 식을 패턴에 맞게 나타낼 수 있습니다. 이러한 작업을 하기 위해서 **token-lexeme 패턴이 필요한 것!!**

<figure><img src="https://velog.velcdn.com/images/dbstjd0924/post/974421b6-56db-45a6-a903-2db87364d7d4/image.png" alt=""><figcaption></figcaption></figure>

{% hint style="warning" %}
즉, 우리가 쓴 코드 한줄한줄에 대해 의미 단위로 끊어주는 역할이라고 생각하면 편합니다.
{% endhint %}

여기서는 문법에 대한 오류는 잡아내지 않습니&#xB2E4;**.(parsing만)**

하지만 <mark style="color:red;">토큰을 추출하는 것만으로는 충분하지 않을 수 있습니다.</mark>

## 5. 복잡한 숫자도 토큰으로 쪼개는 법 – 정규 표현식, 상태기계, 그리고 BNF

### 1) 단순한 '토큰 추출'만으로는 부족하다

프로그래밍 언어는 단지 키워드 몇 개만 인식해서 처리되는 게 아닙니다.\
**실제 언어에서는 다양한 토큰 유형**(변수, 숫자, 연산자 등)이 존재하며, 이를 정교하게 구분할 필요가 있습니다.

* `A+B` 와 `A + B`는 동일한 의미지만 **공백 처리**가 다릅니다.
* 숫자도 단순한 123이 아니라 다음처럼 다양합니다:
  * `1.0`, `.5`, `-3.14`, `+5e10`, `1E-4` …

### 2) 수 하나에도 이렇게 많은 규칙이!?

#### 부동소수점 수 예시

```
1.0, .1, 1e5, -1.2, +1.2e+10, 3E-7 …
```

이들을 모두 포괄하려면, 우리가 흔히 생각하는 숫자 인식보다 훨씬 더 **복잡한 규칙과 흐름**이 필요합니다.

### 3) 이 복잡한 숫자를 인식하는 법: **상태 기계(Finite State Machine)**

#### 상태 기계의 개념

> "한 칸 한 칸 이동하면서 올바른 문자인지 확인하며 경로를 따라가는 방식"

* 각 `원(상태)`은 현재 위치
* `화살표(천이, transition)`는 특정 입력 문자에 대한 이동 경로
* 끝까지 도달하면 → 인식 완료
* 잘못된 문자가 들어오면 → 오류 상태

#### 예: `1.2e+5` 인식 흐름

시작 → 숫자 `1` → `.` → `2` → `e` → `+` → `5` → 종료

> 📍 **그림 8-2**에서는 이 과정을 시각적으로 표현

<figure><img src="../../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

### 4) 부동소수점 수에 대한 상태 테이블

<figure><img src="../../.gitbook/assets/image (3) (1).png" alt=""><figcaption></figcaption></figure>



→ **정상 입력만이 인식되고**, 나머지는 오류 처리됨

### 5) 이걸 공식적으로 표현하는 문법 → BNF

#### 배커스-나우르 표기법 (BNF)

BNF는 문법을 수학적으로 정의하는 고전적인 방법입니다.\
예를 들어 부동소수점 수를 다음처럼 정의합니다:

```bnf
<digit> ::= "0" | "1" | ... | "9"
<digits> ::= <digit> | <digits> <digit>
<e> ::= "e" | "E"
<sign> ::= "+" | "-"
<optional-sign> ::= <sign> | ""
<exponent> ::= <e> <optional-sign> <digits>
<optional-exponent> ::= <exponent> | ""
<mantissa> ::= <digits> | <digits> "." | "." <digits> | <digits> "." <digits>
<floating-point> ::= <optional-sign> <mantissa> <optional-exponent>
```

이런 방식으로 문법을 **엄격하게 정의하고**,\
컴파일러가 정확하게 **올바른 입력인지 판단**할 수 있도록 도와줍니다.

### 6) 상태기계(FSA) vs BNF

| 항목    | 상태기계(FSA) | BNF         |
| ----- | --------- | ----------- |
| 구성    | 상태 + 전이   | 규칙 기반 문법    |
| 표현 방식 | 도해, 전이표   | 수식, 재귀      |
| 용도    | 인식기 구현    | 언어 명세, 규칙화  |
| 예     | 숫자 판별기    | 프로토콜, 구문 규칙 |

> BNF는 **더 넓은 표현력을 가진 형식**,\
> 상태기계는 **더 단순하고 구현 중심적인 방법**입니다.

### 7)상태 기계로 토큰을 분류하는 법 – 리스트 8-4 이해하기

#### 1️⃣ 상태 기계란 무엇이었죠?

앞서 배운 상태 기계(Finite State Machine)는 입력을 한 글자씩 따라가며 **상태를 변화시키는 방식**으로 작동합니다.\
즉, "지금 상태가 이렇고, 입력이 이거라면 다음 상태는 이거야!" 를 기록한 것이 `상태 전이표(state transition table)`입니다.

#### 2️⃣ 어떻게 작동하나요?

책에서는 아래와 같은 간단한 코드 예시를 보여줍니다:

```c
state = 1;
while (state > 0)
    state = state_table[state][next_character];
```

#### 📌 작동 원리 요약

| 코드 요소                                | 의미                                    |
| ------------------------------------ | ------------------------------------- |
| `state = 1`                          | 시작 상태 지정 (보통 초기 상태는 1)                |
| `state_table[state][next_character]` | 현재 상태와 입력 문자를 기준으로 다음 상태를 계산          |
| `while (state > 0)`                  | 상태가 0보다 클 때까지만 진행 (0은 "완료", 음수는 "오류") |

#### 🧠 예시

예를 들어 `+1.2e-3` 같은 입력을 받는다고 가정하면:

```
상태 1 → '+' → 상태 2  
상태 2 → '1' → 상태 3  
상태 3 → '.' → 상태 4  
상태 4 → '2' → 상태 4  
상태 4 → 'e' → 상태 5  
상태 5 → '-' → 상태 6  
상태 6 → '3' → 상태 6  
```

마지막 상태가 6이고, 더 이상 읽을 문자가 없다면 종료!

#### 3️⃣ 완료'와 '오류' 상태는 어떻게 다를까요?

책에서는 상태 전이표를 다음과 같이 단순화합니다:

* **완료 상태** → `0`
* **오류 상태** → `-1`
* 이외 상태 → 계속 진행

즉, `while (state > 0)`라는 코드는 <mark style="color:red;">완료(0) 또는 오류(-1)를 만나면 루프를 빠져나옵니다.</mark>

#### 4️⃣ 다양한 토큰으로 확장하는 방법

리스트 8-4는 단순히 "부동소수점 수"만 처리하지만, 이 방식은 **다른 모든 토큰 유형에도 응용 가능**합니다.

예를 들어:

* 숫자 → 종료 상태 0
* 변수 이름 → 종료 상태 100
* 연산자 → 종료 상태 200
* 괄호 → 종료 상태 300

처럼 **각기 다른 종료 상태 값**을 줌으로써 **무엇을 인식했는지 구분**할 수 있게 됩니다.

#### 확장 예시:

```c
while (state > 0)
    state = state_table[state][next_character];

if (state == 0)
    token_type = "float";
else if (state == 100)
    token_type = "identifier";
else if (state == -1)
    error("잘못된 토큰입니다!");
```

> 💬 **한 줄 요약**\
> “상태 기계는 복잡한 입력도 간단한 테이블과 루프로 인식할 수 있게 해주는 구조적 도구다.

#### 요약

| 개념         | 설명                       |
| ---------- | ------------------------ |
| 상태기계       | 상태를 이동하며 입력을 분석          |
| 부동소수점 수    | 다양한 형식 존재 → 복잡한 인식 과정 필요 |
| BNF        | 문법을 형식적으로 표현하는 방법        |
| FSM vs BNF | 구현 vs 정의 중심의 도구          |

> 💬 **한 줄 정리**\
> “프로그래밍 언어는 단순한 문자 추출로 끝나지 않는다. 복잡한 숫자 표현과 문법은 상태기계와 BNF로 정교하게 다뤄야 한다.”

### **8)** 복잡한 숫자도 간단히 분석하는 법 – 정규표현식과 Lex의 세계 <a href="#undefined" id="undefined"></a>

#### 1️⃣ 왜 상태 전이표만으로는 부족할까?

부동소수점 수처럼 복잡한 토큰을 상태기계로 표현하려면,

* 수많은 상태를 정의해야 하고,
* 전이 경로도 많아져서,
* **직접 표를 짜는 게 매우 지루하고 오류도 잘 발생**합니다.

> 해결책은?\
> &#xNAN;**"문법을 직접 만들지 말고, 정규표현식이라는 언어로 언어를 기술하자!"**

#### 2️⃣ 정규표현식의 탄생과 역사

#### 📜 간략 역사 흐름

| 연도       | 인물               | 기여                      |
| -------- | ---------------- | ----------------------- |
| **1956** | 스티븐 클레이니         | 정규표현식 개념의 수학적 정의        |
| **1968** | 켄 톰슨             | 텍스트 편집기에 정규표현식 기능 도입    |
| **1975** | 마이크 레스크 & 에릭 슈미트 | `lex` 도구 개발 → 유닉스에서 대중화 |
| 이후       | GNU에서 `flex` 개발  | 오픈소스 버전                 |

> 🎵 비틀즈의 "It's a clean machine"이라는 가사에서 "Kleene machine"이라는 언어 유희도 등장합니다.

#### 3️⃣ 🎯 정규표현식이란?

정규표현식은 우리가 평소에 문자열 찾을 때 쓰는 그 **regex**와 같은 개념이에요.

![](https://velog.velcdn.com/images/dbstjd0924/post/1d25b9ca-1adb-436d-8be5-03039d8e2b20/image.png)

예를 들어,

* 정수는 `[0-9]+` (0\~9 숫자가 하나 이상 반복)
* 식별자는 `[a-zA-Z_][a-zA-Z0-9_]*` (문자 또는 \_로 시작하고, 그 뒤에 숫자 또는 문자가 올 수 있음)
* 공백은 `[ \t\n\r]+`

이런 식으로 각 **lexeme(어휘 단위)**&#xAC00; 가져야 할 패턴을 정규표현식으로 정의해요.

정리하면,

| 타입            | 정규표현식 예시                 | 의미 설명                     |
| ------------- | ------------------------ | ------------------------- |
| `int_literal` | `[0-9]+`                 | 정수는 숫자 하나 이상              |
| `id` (식별자)    | `[a-zA-Z_][a-zA-Z0-9_]*` | 변수 이름이나 키워드로 사용될 수 있는 문자열 |
| `keyword`     | \`if                     | else                      |

정규표현식 덕분에 **문자열이 어떤 역할을 할지를 자동으로 판단**할 수 있는 거예요.

**🔍 정규표현식은 왜 중요할까?**

정규표현식은 단지 문자열을 찾는 도구가 아니라, **프로그래밍 언어에서 '어떤 단어가 어떤 역할을 할지'를 정해주는 기준**이기도 해요.\
어휘 분석기는 이 정규표현식을 기준으로 소스코드를 스캔하며, 규칙을 벗어나는 경우엔 에러를 발생시킵니다.

📌 예를 들어, `2hello`는 변수명 규칙에 맞지 않기 때문에 에러!

> **정규 표현식**은 수학적으로 정의된 기호와 연산을 이용하여 언어를 귀납적으로 정의하기 위한 방법입니다.&#x20;

**✅ 주요 기호 요약**

| 기호      | 의미         |
| ------- | ---------- |
| `.`     | 아무 글자 하나   |
| `*`     | 0번 이상 반복   |
| `+`     | 1번 이상 반복   |
| `?`     | 0번 또는 1번   |
| `[...]` | 문자 집합      |
| \`      | \`         |
| `(...)` | 그룹화        |
| `\`     | 특수문자 이스케이프 |

#### 4️⃣ 📌 부동소수점 수를 표현한 정규표현식 (그림 8-3)

<figure><img src="../../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

```regex
[+-]?(([0-9]*\.?[0-9]+)|([0-9]+\.?[0-9]*))([Ee][+-]?[0-9]+)?
```

#### 📖 의미 해석 (왼→오 순서)

1. `[+-]?`\
   → **선택적인 부호**
2. `(([0-9]*\.?[0-9]+)|([0-9]+\.?[0-9]*))`\
   → **소수 표현 두 가지 방식 중 하나**
3. `([Ee][+-]?[0-9]+)?`\
   → **선택적인 지수 표현**

> ✅ 정수, 소수, 과학적 지수 표기법까지 모두 포함!

#### 5️⃣ 🛠 Lex: 정규식을 코드로 자동 생성해주는 도구

#### Lex란?

* **어휘 분석기(lexical analyzer)** 생성 도구
* 정규표현식 기반으로 **토큰 분류 자동화**

#### 📋 예시&#x20;

```lex
[+-]?[0-9]+                                 return (INTEGER);
[+-]?(([0-9]*\.?[0-9]+)|([0-9]+\.?[0-9]*))([Ee][+-]?[0-9]+)?   return (FLOAT);
[A-Za-z][A-Za-z0-9]*                        return (VARIABLE);
\+                                          return (PLUS);
\*                                          return (TIMES);
\/                                          return (DIVIDE);
=                                           return (EQUALS);
```

→ 각 패턴에 대해 **정해진 동작** 또는 **토큰 반환값**을 지정할 수 있음!

#### 6️⃣ 💬 재미있는 예시: 보스턴 사투리 변환기&#x20;

```lex
[ae]r              printf("ah");
a/[ .,;!?]         printf("er");
```

#### 작동 방식

* `ar`, `er`을 `ah`로 바꿈
* `a`가 마침표나 공백으로 끝날 경우 `er`로 바꿈

> 🔊 `"Park the car"` → `"Pahk the cah"`

#### 7️⃣ 📊 FSM, Regex, Lex 비교 요약

| 도구/개념     | 핵심 역할              | 장점            | 단점                       |
| --------- | ------------------ | ------------- | ------------------------ |
| 상태기계(FSM) | 문자별 상태 전이 추적       | 구현 단순         | 표현 복잡                    |
| 정규표현식     | 패턴을 간단히 기술         | 표현력 좋음        | 복잡한 언어에는 한계              |
| Lex/Flex  | 정규식 → 어휘 분석기 자동 생성 | 자동화, 유지 보수 용이 | 일부 언어에 부적합 (ex. Fortran) |

#### 📍 정수인지 문자(key)인지, 정규표현식으로 어떻게 구분할까?

프로그래밍 언어나 컴파일러를 만들 때, 우리는 소스 코드를 "토큰" 단위로 쪼개는 작업을 해요. 이걸 **어휘 분석(Lexical Analysis)**&#xC774;라고 하죠.

예를 들어 다음과 같은 코드가 있다고 해봅시다.

```c
x = 2;
```

여기서 `x`는 변수 이름이고 `=`는 대입 연산자이며 `2`는 정수입니다.

<figure><img src="https://velog.velcdn.com/images/dbstjd0924/post/9721317e-9bb9-4cb2-a1c1-df81253d6cd4/image.png" alt=""><figcaption></figcaption></figure>

그런데 컴퓨터는 처음엔 이걸 그저 **문자의 나열**, 즉 `'x'`, `'='`, `'2'` 이렇게밖에 몰라요.\
그래서 중요한 문제가 생깁니다.

> ❓ **"2는 문자일까? 정수일까? 혹은 변수 이름일까?"**

이걸 구분해주기 위해 `정규표현식(Regular Expression)`이 사용됩니다.

정규 표현식에서 정의되는 연산은 접합, 클린 클로저, OR이 있으며, 피연산자는 기호의 유한 집합인 알파벳이나 기호가 됩니다.

말이 어렵게 되어있지만 그냥 우리가 아는 문자열 정규식이랑 동일해요. 정규식안에는 각 lexeme가 가져야하는 규칙이 들어가 있고 그 **규칙을 벗어나는 경우에는 에러를 발생시킵니다!**

> 💬 **한 줄 정리**\
> “정규표현식은 복잡한 문자를 간단히 표현하고, Lex는 이 표현을 프로그램으로 자동화한다.”

### 9) Lex가 모든 언어에 적용되지는 않는다

#### 1️⃣ Lex, 정말 좋은데… 못 쓰는 경우도 있다?

`lex`는 정규표현식을 기반으로 **어휘 분석기(Lexical Analyzer)를 자동 생성**해주는 강력한 도구입니다.\
그러나, **모든 언어의 어휘 구조가 정규식으로 표현 가능한 것은 아닙니다.**

#### 2️⃣ 💬 스티븐 C. 존슨의 말

> “렉스를 사용하면 아주 복잡한 어휘 분석기도 쉽게 만들 수 있지만,\
> 포트란처럼 어떤 이론적인 프레임워크에 어휘 체계가 들어맞지 않는 언어가 몇 가지 있고,\
> 이런 언어의 어휘 분석기는 **어쩔 수 없이 손으로 작성**하는 수밖에 없다.”

#### 3️⃣ 🧠 왜 lex가 안 되는 언어가 있을까?

#### ✔ lex 기반 어휘 분석기의 전제 조건

* 정규 표현식 기반(Finite State Automata 수준)
* **문맥에 따라 변화하지 않는 패턴**일 것
* **앞에서의 입력이 뒤 해석에 영향 미치지 않아야 함**

#### ❌ lex로 어려운 경우 예시

| 언어                      | 문제점                                                                |
| ----------------------- | ------------------------------------------------------------------ |
| **Fortran**             | **공백에 의미가 있음** → `DO 10 I = 1,10`은 루프 / `DO10I = 1.10`은 대입문        |
| **C/C++**               | `typedef`에 따라 **토큰 의미가 바뀜** (예: `foo`가 타입인지 변수인지 구문 분석 전까지 알 수 없음) |
| **의미 기반 전방 탐색이 필요한 언어** | 예: Python의 들여쓰기 기반 구조 파악 등                                         |

#### &#x20;4️⃣ 정리

| 항목      | 설명                                           |
| ------- | -------------------------------------------- |
| lex의 장점 | 정규식으로 복잡한 어휘 분석기 자동 생성 가능                    |
| 제한점     | **문맥 민감한 언어**, **비정형 공백 처리** 등은 정규식만으로 해결 불가 |
| 해결책     | 일부 언어는 **수작업(hand-written)** 어휘 분석기 필요       |

***

> 💬 **한 줄 요약**\
> “lex는 강력하지만 모든 언어를 처리할 수 있는 만능 열쇠는 아니다. 언어의 어휘 구조가 단순할수록 lex의 효과는 커진다.”

### 10) 보충정리 : 자바의 어휘 분석기 생성기

&#x20;자바에서 직접적으로 `lex`라는 이름의 도구는 존재하지 않지만, **`lex`와 같은 역할을 수행하는 도구들**이 존재합니다. 이를 일반적으로 "어휘 분석기 생성기(Lexical Analyzer Generator)"라고 부르며, 자바에서는 대표적으로 다음과 같은 도구들이 널리 사용됩니다.

#### 자바에서 Lex를 대체하는 도구들

**1) JFlex**

* Lex의 **자바 버전**이라 할 수 있는 가장 유명한 도구
* 정규표현식을 기반으로 **Java 코드로 어휘 분석기를 자동 생성**
* `yylex()` 같은 방식으로 토큰을 순차적으로 반환
* `.flex` 파일을 작성해서 Java 소스로 컴파일

> 💡 JFlex는 "Java + Flex"의 합성어

**예시**

```flex
%%
[0-9]+      { return new Token(INTEGER, yytext()); }
[a-zA-Z]+   { return new Token(IDENTIFIER, yytext()); }
"+"         { return new Token(PLUS, yytext()); }
```

2\) **ANTLR (또는 ANTLR4)**

* **어휘 분석 + 구문 분석까지 한 번에** 해주는 매우 강력한 도구
* Lex/Yacc 스타일과는 좀 다르지만 **자바 지원이 매우 우수**
* `.g4` 문법 파일을 작성해 **토큰과 구문을 모두 자동 생성**

**예시**

```antlr
fragment DIGIT : [0-9];
INT : DIGIT+;
ID  : [a-zA-Z]+;
PLUS: '+';
```

**🧾 정리**

| 항목        | 설명                                          |
| --------- | ------------------------------------------- |
| **Lex**   | C 기반의 어휘 분석기 생성기                            |
| **JFlex** | Java용 Lex 역할을 수행 (정규식 기반, `Scanner` 클래스 생성) |
| **ANTLR** | 어휘 + 구문 분석까지 가능 (더 복잡한 언어 처리에 적합)           |

> 💬 **한 줄 요약**\
> “자바에서는 `JFlex`가 Lex의 자바 버전 역할을 하고, 더 강력한 분석이 필요할 땐 `ANTLR`을 쓴다.”

`JFlex`나 `ANTLR` 의 사용처 및 위치

**🔍 왜 JVM에 포함되지 않았을까?**

#### 1) JVM은 **런타임 환경**

* JVM은 컴파일된 바이트코드(.class)를 실행하는 엔진이에요.
* 즉, Java 프로그램을 **실행**하는 데 필요한 도구와 라이브러리만 포함되어 있습니다.

#### 2) `JFlex`나 `ANTLR`은 **개발 도구**

* 이들은 "소스코드 작성 → 어휘 분석기 생성"이라는 **개발 단계**에서 사용됩니다.
* `javac` 컴파일러처럼 **빌드 타임 도구**에 가깝기 때문에 JVM에 기본 포함되지 않습니다.



**🧰 그러면 어디서 쓰는 걸까?**

| 도구        | 사용 시점                  | 설치 방법                                     |
| --------- | ---------------------- | ----------------------------------------- |
| **JFlex** | 소스코드에서 어휘 분석기 클래스 생성 시 | JAR 파일 다운로드 또는 Maven/Gradle 등록            |
| **ANTLR** | 어휘 + 구문 분석기 자동 생성 시    | ANTLR JAR 또는 플러그인 사용 (Gradle, IntelliJ 등) |

**Maven 예시 (JFlex)**

```xml
<dependency>
  <groupId>de.jflex</groupId>
  <artifactId>jflex</artifactId>
  <version>1.8.2</version>
</dependency>
```

**요약**

| 질문                                  | 답변                                |
| ----------------------------------- | --------------------------------- |
| Lex(JFlex, ANTLR)가 JVM 안에 포함되어 있나요? | ❌ 아닙니다.                           |
| 왜 포함 안 됐나요?                         | JVM은 실행기, Lex는 **개발 도구**입니다.      |
| 그럼 어떻게 쓰나요?                         | 별도로 설치하거나 Maven/Gradle로 추가해야 합니다. |

> 💬 **한 줄 요약**\
> “JFlex와 ANTLR은 JVM에 포함되어 있지 않으며, 개발 시점에 따로 설치해서 사용하는 도구입니다.”

***

## 6. Lex로 단어를, Yacc으로 문장을 이해하자 – 간단한 계산기 구현 흐름

### 1) Lex에서 끝나지 않는 이유

Lex는 **문자 시퀀스를 토큰(단어)으로 바꾸는 도구**입니다.

하지만 예를 들어 보세요:

* `1 + 2` → 맞는 문장 (OK)
* `1 + + + 2` → 틀린 문장 (에러)

Lex는 각 `+`나 숫자를 **그냥 토큰으로만 구분**할 수 있을 뿐, **그 조합이 문법적으로 올바른 문장인지 확인하지 못합니다.**

### 2) ✨ Yacc의 등장: 문장을 파악하는 도구

이런 **토큰들의 순서를 검사**하는 도구가 바로 `Yacc (Yet Another Compiler Compiler)`입니다.

<figure><img src="../../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

* Lex가 만든 **토큰 시퀀스**를 입력으로 받아
* **BNF(Backus-Naur Form) 문법**에 따라
* **문장이 맞는지 판단하고 실행**까지 이어주는 도구입니다.

GNU에서는 이와 유사한 오픈소스 도구 **Bison**도 제공하고 있습니다.

### 3) 🧱 Yacc은 어떻게 문장을 판단할까?

Yacc이 만들어내는 파서는 **스택 기반의 시프트-리듀스 파서(Shift-Reduce Parser)**&#xC785;니다.

#### 💡 개념 정리

| 용어         | 의미                              |
| ---------- | ------------------------------- |
| **Shift**  | 토큰을 스택에 쌓는 동작                   |
| **Reduce** | 스택의 토큰 조합이 어떤 문법 규칙에 맞으면 하나로 줄임 |

#### 🔄 예: `4 + 5 - 3`을 처리하는 과정 (그림 8-4)

<figure><img src="../../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

```
[입력] 4 + 5 - 3

Shift:  4       → 스택: [4]
Shift:  +       → 스택: [4, +]
Shift:  5       → 스택: [4, +, 5]
Reduce: + 적용 → 스택: [9]
Shift:  -       → 스택: [9, -]
Shift:  3       → 스택: [9, -, 3]
Reduce: - 적용 → 스택: [6]
```

결과: ✅ 문법에 맞는 수식이며 최종값은 `6`

### 4) 🧾 리스트 8-7: 계산기의 BNF

```bnf
<operator>   ::= PLUS | MINUS | TIMES | DIVIDE
<operand>    ::= INTEGER | FLOAT | VARIABLE
<expression> ::= <operand>
               | <expression> <operator> <operand>
<assignment> ::= VARIABLE EQUALS <expression>
<statement>  ::= <expression> | <assignment>
<statements> ::= "" | <statements> <statement>
<calculator> ::= <statements>
```

{% hint style="danger" %}
→ Lex로 `INTEGER`, `PLUS` 등의 토큰을 만들고,\
→ Yacc은 그것들을 위 문법에 맞게 조립해 **전체 프로그램을 분석**합니다.
{% endhint %}

### 5) 🧪 리스트 8-8: 계산기 Yacc 코드 일부

```yacc
calculator : statements ;

statements : /* 비어 있음 */
           | statement statements ;

operand : INTEGER
        | FLOAT
        | VARIABLE ;

expression : expression PLUS operand
           | expression MINUS operand
           | expression TIMES operand
           | expression DIVIDE operand
           | operand ;

assignment : VARIABLE EQUALS expression ;

statement : expression
          | assignment ;
```

> Lex로부터 넘겨받은 토큰을 이 BNF 구조에 맞춰 처리합니다.

### 📌 요약 정리

| 단계    | 도구       | 역할                                                |
| ----- | -------- | ------------------------------------------------- |
| 어휘 분석 | **Lex**  | 문자 시퀀스를 토큰으로 분리 (`INTEGER`, `PLUS`, `VARIABLE` 등) |
| 문법 분석 | **Yacc** | 토큰 시퀀스를 BNF 문법에 따라 문장으로 조립                        |
| 실행    | 파서가 판단   | 시프트-리듀스를 통해 계산, 해석, 실행 등 수행                       |

> 💬 **한 줄 요약**\
> “Lex는 단어를, Yacc은 문장을 분석하며, 이 둘을 통해 컴파일러의 앞단(프론트엔드)이 완성된다.”

***

## 7. Syntax Analysis 정리: 문장을 이해하는 문법 분석기

### 1) Syntax Analysis란?

#### 📌 정의

`Syntax Analysis(구문 분석)는` **Lexical Analysis** 단계에서 얻은 토큰들을 가지고, **문법에 맞게 구조를 분석**하는 단계입니다.\
이 과정에서 **파스 트리(Parse Tree)**&#xB97C; 구성하게 됩니다.

#### 💬 쉽게 말하면

> “단어(token)는 정리됐고, 이제 문장으로 맞게 조합되는지를 검사하는 단계!”

### 2) Context-Free Grammar (CFG): 문장을 만드는 설계도

#### 📌 정의

`CFG(문맥 자유 문법)`은 **파스 트리를 만들기 위한 문법 체계**입니다.\
여기서 규칙을 정의하는 방식은 아래와 같습니다:

```
<stmt> → <var> = <expr> ;
<expr> → <expr> + <expr> | <expr> * <expr> | <var> | <const>
```

#### 🛠 구성요소

* **Terminal**: 실제 문자열로 변환될 수 있는 기호 (`X`, `Y`, `+`, `*`, `=`, `;`)
* **Non-terminal**: 아직 다른 규칙으로 바꿀 수 있는 기호 (`<stmt>`, `<expr>`, `<var>` 등)

#### 💬 쉽게 말하면

> `stmt`는 "문장을 만드는 규칙", `expr`는 "표현식을 만드는 방식"을 나타냅니다.

### 3) Derivation: 문장을 만들어가는 여정

#### 📌 정의

**Derivation**은 non-terminal들을 하나씩 바꿔가며 **terminal로 이루어진 문장을 완성하는 과정**입니다.

> “규칙에 따라 문장을 점점 만들어가는 단계!”

#### ✍ 예시: `X = Y - Y * X` 만들기

우리는 다음 규칙을 따른다고 가정합니다:

```
<stmt_list> → <stmt>  
<stmt> → <var> = <expr>  
<expr> → <expr> + <expr> | <expr> * <expr> | <expr> - <expr> | <var>  
<var> → X | Y  
```

**📐 Derivation 과정:**

```
<stmt_list>
→ <stmt>
→ <var> = <expr>
→ X = <expr>
→ X = <expr> - <expr>
→ X = <var> - <expr>
→ X = Y - <expr> * <expr>
→ X = Y - <var> * <var>
→ X = Y - Y * X
```

이렇게 단계적으로 non-terminal을 terminal로 바꾸며 **문장을 완성**하는 과정이 바로 derivation입니다.

> 💬 **한 줄 요약**\
> “Syntax Analysis는 문장의 구조를 검사하는 단계이며, CFG를 통해 파스 트리를 만들고, derivation을 통해 문장을 완성해나간다.”

***

## 8. 파싱과 파스 트리: 문법적으로 해석하기

어휘 분석으로 토큰을 얻었으면, 이제 이 토큰들이 **문법적으로 맞는지 파악**해야 합니다. 이 과정을 \*\*파싱(parsing)\*\*이라고 합니다.

#### 파스 트리(Parse Tree)

토큰들의 구조를 **트리 형태**로 표현한 것

예: `1 + 2 * 3`

```
      +
     / \
    1   *
       / \
      2   3
```

이 트리를 구성하면 컴파일러나 인터프리터는 이 구조를 **실행하거나 최적화**할 수 있게 됩니다
