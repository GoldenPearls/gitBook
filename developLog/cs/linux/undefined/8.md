---
icon: '8'
---

# 8장 : 메모리 계층

## 🧠 컴퓨터의 메모리 계층 구조와 캐시 메모리의 모든 것

### 1. 기억 장치 계층 구조란?

<figure><img src="../../../.gitbook/assets/image.png" alt=""><figcaption><p><a href="https://velog.io/@ajm0718/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B5%AC%EC%A1%B0">https://velog.io/@ajm0718/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B5%AC%EC%A1%B0</a></p></figcaption></figure>

컴퓨터 내부에는 데이터를 저장하고 불러오는 다양한 저장 장치들이 있습니다. 이들을 성능과 특성에 따라 정리한 것이 바로 "기억장치 계층 구조"입니다. 이 계층은 보통 위로 갈수록 **작고 빠르며 비싸고**, 아래로 갈수록 **크고 느리며 싸다**는 특징을 가집니다.

<figure><img src="../../../.gitbook/assets/image (1).png" alt=""><figcaption><p><a href="https://blog.naver.com/gimhaeoa/221933664741?viewType=pc">https://blog.naver.com/gimhaeoa/221933664741?viewType=pc</a></p></figcaption></figure>

* **레지스터**: CPU 명령어 실행에 직접 사용되는 가장 빠른 저장 장치
* **캐시 메모리 (L1, L2, L3)**: CPU 속도를 보조하는 중간 저장소
* **메인 메모리 (RAM)**: 실행 중인 프로그램과 데이터를 저장하는 공간
* **보조 저장장치 (SSD, HDD)**: 전원을 꺼도 유지되는 장기 저장 장치

이 구조 덕분에 **속도와 비용, 효율성**을 모두 고려한 데이터 처리 시스템이 구현됩니다.

### 2. CPU는 왜 캐시 메모리를 사용할까?

CPU는 명령어를 처리할 때 다음의 기본 사이클을 따릅니다:

1. **명령어 Fetch**: 메모리에서 명령어를 읽어온다
2. **Operand Fetch**: 명령어 실행에 필요한 데이터를 메모리에서 가져온다
3. **Execute**: 레지스터에서 연산을 수행한다
4. **Write Back**: 결과를 다시 메모리에 기록한다

하지만 이 중 **Fetch 단계**가 매우 느립니다. 레지스터에서 처리하는 계산 시간에 비하면 메모리 접근 속도는 무척 느립니다. <mark style="color:red;">메모리 접근이 CPU 연산보다 훨씬 오래 걸리기 때문이죠.</mark>&#x20;

EX. 어떤 PC에서는 레지스터 계산은 1회당 약 1나노초 미만이지만, 메모리 접근은 1회당 수십 나노초가 걸립니다. 따

> 그래서 등장한 것이 **캐시 메모리**입니다.&#x20;

캐시는 자주 쓰는 데이터를 메모리 대신 저장하고, 훨씬 더 빠르게 접근할 수 있게 도와주는 **중간 저장소**입니다.

### 3. 캐시 메모리의 동작 원리

#### 캐시 라인(Cache Line) 단위로 저장

캐시는 메모리 주소 1개만 가져오는 것이 아니라, `일정 크기의 데이터를 묶음(Cache Line)`으로 가져옵니다. 캐시 메모리에 캐시라인이라고 부르는 단위로 <mark style="color:red;">데이터를 읽어서 그 데이터를 레지스터로 옮깁니다.</mark> 예: 64Byte

#### 가상적인 CPU로 캐시 메모리를 살펴보자.

* 레지스터는 RO과 R1 두 종류이며, 둘 다 크기는 10바이트
* 캐시 메모리 크기는 50바이트
* 캐시라인 크기는 10바이트

1. CPU의 RO에 메모리 주소 300에 접근&#x20;

<figure><img src="../../../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>



2. CPU가 주소 300의 데이터를 다시 읽는다면, 예를 들어 RI으로 읽어 온다면 메모리에서 가져오   는 대신에 <mark style="color:red;">캐시 메모리에 곧바로 접근하면 되니까</mark> 빠르게 처리가 가능합니다.

<figure><img src="../../../.gitbook/assets/image (3).png" alt=""><figcaption></figcaption></figure>

3. RO 값을 변경하고 변경된 내용을 메모리 주소 300에 반영한다면 메모   리에 쓰기 전에 캐시 메모리에 먼저 저장합니다.&#x20;

이때 캐시라인에는 <kbd><mark style="background-color:blue;">메모리에서 읽어 들인 데이터가 변경되었다는 것을 뜻하는 표시<mark style="background-color:blue;"></kbd>를 붙입니다. 이런 표시가 붙은 캐시라인을 **더티하다**라고 이야기합니다

<figure><img src="../../../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

#### 더티 비트(Dirty Bit)

* CPU가 값을 바꾸면 **캐시에서 먼저 값이 바뀜**
* 나중에 이 변경된 값을 **메모리에 반영**할 때만 쓰기 작업 수행
* 이때 더티 비트를 체크해, 실제로 바뀐 것만 쓰도록 함

#### 메모리에 데이터를 쓰는 방법: 라이트 스루 vs 라이트 백

| 전략            | 설명                        | 장점        | 단점           |
| ------------- | ------------------------- | --------- | ------------ |
| Write-through | 캐시에 저장하면서 동시에 메모리에도 저장    | 데이터 유실 없음 | 느림           |
| Write-back    | 일단 캐시에만 저장하고, 나중에 메모리에 저장 | 빠름        | 전원 꺼지면 손실 위험 |

4. 더티 표시 붙은 캐시라인을 메모리에 반영하기

<figure><img src="../../../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

캐시 메모리가 가득 찼는데 **캐시에 존재하지 않는 데이터를 읽어 들이면** 기존의 캐시라인 중에서 하나를 버리고 **빈 캐시라인에 새로운 데이터를 넣습니다.**



5. 캐시 메모리가 가득 찬 상태일 때 캐시라인에서 데이터 버리기

<figure><img src="../../../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>

예시로주소&#x20;350의 데이터를 읽으면 캐시라인의 데이터 하나를 버리고(주소 340-350 필드)



6. 새로운 데이터를 캐시라인에 복사

<figure><img src="../../../.gitbook/assets/image (7).png" alt=""><figcaption></figcaption></figure>

지금 비운 캐시&#x20;라인에 읽어 올 주소의 데이터를 복사합니다.

이때 버리는 **캐시라인이 더티 상태**라면 메모리에 데이터를 저장하는 **클린 처리**를 하고 **버립니다.**

### 4. 캐시 교체 알고리즘과 스래싱

캐시 메모리는 용량이 작기 때문에, **자주 사용하는 데이터만 유지**하고 오래 안 쓰는 것은 버립니다. 이를 위한 알고리즘이 대표적으로 **LRU(Least Recently Used)** 등입니다.

하지만 어떤 프로그램이 너무 많은 메모리 범위를 널뛰듯 접근하면, 캐시가 **계속 버리고 읽는 상태**가 되면서 성능이 급감하는 **스래싱(thrashing)** 현상이 발생합니다.

### 5. 캐시의 작동원리 : 지역성(Locality)의 법칙

> "미리 쓸만한 데이터를 메인 메모리에서 캐시에 옮겨놓자"

캐시 메모리를 관통하는 핵심 개념은 데이터 지역성 (Locality) 입니다. 캐시 메모리는 이 원리로 작동합니다

1. **시간 지역성 (Temporal locality)**: 최근에 사용한 데이터를 또 사용할 가능성이 높다.
2. **공간 지역성 (Spatial locality)**: 지금 접근한 데이터 근처도 곧 사용할 가능성이 높다.
3. **순차적 지역성( Sequential Locality)** : 비순차적 실행이 아닌 이상 명령어들이 메모리에 저장된 순서대로 실행되는 특성을 고려하여 다음 순서의 데이터가 곧 사용될 가능성이 높다.

예를 들어, 공간 지역성은 A\[0], A\[1] 로 구성되는 배열과 같이 참조된 데이터 근처의 데이터가 잠시후에 사용될 가능성이 높다는 것입니다.

반복문 돌릴 때 i, i+1, i+2 이런 식으로 접근하잖아요? 이걸 미리 예측해서 데이터를 캐시에 담아두는 겁니다.

결국 캐시는 이렇게 생각하면 돼요:

> "CPU가 가장 아끼는 친구! 항상 옆에 붙어 다니는 비서 같은 존재"

### 6. 캐시 계층 구조 (L1 / L2 / L3)

#### L1 캐시

* CPU 내부에 존재
* 매우 빠르고 작음 (32KB \~ 64KB)
* 명령어와 데이터를 구분해 저장하는 경우도 있음

#### L2 캐시

* CPU 칩 내부 또는 외부
* 속도는 L1보다 느리지만, 용량이 큼 (256KB \~ 512KB)

#### L3 캐시

* 여러 코어가 공유하는 캐시
* 수 MB\~수십 MB 수준의 용량

***

### 7. 메모리 지역성 원리

프로그램은 데이터를 무작위로 접근하지 않습니다. 대부분 다음과 같은 패턴을 보입니다:

* **시간적 지역성(Temporal locality)**: 최근에 사용한 데이터는 가까운 미래에 또 사용된다\
  (예: 반복문 안 변수)
* **공간적 지역성(Spatial locality)**: 한 번 접근한 메모리 근처가 다음에 또 사용된다\
  (예: 배열 순회)

이런 원리를 이용해 캐시 효율을 극대화할 수 있습니다.

***

### 8. 메모리 접근 성능 실험(cache.go)

#### 실험 개요

* 점점 더 큰 버퍼(4KB\~64MB)를 만들고
* 각 캐시라인을 순차적으로 접근
* 접근 횟수 대비 소요 시간 측정

#### 결과

* **작은 버퍼에서는 매우 빠름 (L1 캐시 내에서 동작)**
* **점점 버퍼가 커질수록 속도 느려짐 (L2→L3→RAM 이동)**
* **그래프에서 ‘계단형’ 성능 변화가 발생**

이 실험을 통해 **캐시의 존재와 효용성**을 수치로 증명할 수 있습니다.

***

### 9. 결론: 메모리 계층 구조의 존재 이유

왜 이렇게 복잡한 메모리 계층을 만들었을까요?

> "빠르고 비싼 기억장치는 작게, 느리고 저렴한 기억장치는 크게."

이 전략을 통해 우리는 컴퓨터 시스템에서

* **속도** (캐시)
* **유지 비용** (RAM/디스크)
* **유연성** (계층형 메모리 활용)\
  을 모두 확보할 수 있게 된 것입니다.

***

이후 필요하다면 다음 주제로 이어서 설명 가능합니다:

* MMU와 가상 메모리 구조
* 페이지 교체 알고리즘 (LRU, Clock 등)
* 캐시 일관성(Coherency)
* NUMA 구조와 캐시 계층 영향 등

다음 주제로 연결해 드릴까요?
