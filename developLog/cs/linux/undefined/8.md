---
icon: '8'
---

# 8장 : 메모리 계층

## 🧠 컴퓨터의 메모리 계층 구조와 캐시 메모리의 모든 것

### 1. 기억 장치 계층 구조란?

<figure><img src="../../../.gitbook/assets/image.png" alt=""><figcaption><p><a href="https://velog.io/@ajm0718/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B5%AC%EC%A1%B0">https://velog.io/@ajm0718/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B5%AC%EC%A1%B0</a></p></figcaption></figure>

컴퓨터 내부에는 데이터를 저장하고 불러오는 다양한 저장 장치들이 있습니다. 이들을 성능과 특성에 따라 정리한 것이 바로 "기억장치 계층 구조"입니다. 이 계층은 보통 위로 갈수록 **작고 빠르며 비싸고**, 아래로 갈수록 **크고 느리며 싸다**는 특징을 가집니다.

<figure><img src="../../../.gitbook/assets/image (1).png" alt=""><figcaption><p><a href="https://blog.naver.com/gimhaeoa/221933664741?viewType=pc">https://blog.naver.com/gimhaeoa/221933664741?viewType=pc</a></p></figcaption></figure>

* **레지스터**: CPU 명령어 실행에 직접 사용되는 가장 빠른 저장 장치
* **캐시 메모리 (L1, L2, L3)**: CPU 속도를 보조하는 중간 저장소
* **메인 메모리 (RAM)**: 실행 중인 프로그램과 데이터를 저장하는 공간
* **보조 저장장치 (SSD, HDD)**: 전원을 꺼도 유지되는 장기 저장 장치

이 구조 덕분에 **속도와 비용, 효율성**을 모두 고려한 데이터 처리 시스템이 구현됩니다.

### 2. CPU는 왜 캐시 메모리를 사용할까?

CPU는 명령어를 처리할 때 다음의 기본 사이클을 따릅니다:

1. **명령어 Fetch**: 메모리에서 명령어를 읽어온다
2. **Operand Fetch**: 명령어 실행에 필요한 데이터를 메모리에서 가져온다
3. **Execute**: 레지스터에서 연산을 수행한다
4. **Write Back**: 결과를 다시 메모리에 기록한다

하지만 이 중 **Fetch 단계**가 매우 느립니다. 레지스터에서 처리하는 계산 시간에 비하면 메모리 접근 속도는 무척 느립니다. <mark style="color:red;">메모리 접근이 CPU 연산보다 훨씬 오래 걸리기 때문이죠.</mark>&#x20;

EX. 어떤 PC에서는 레지스터 계산은 1회당 약 1나노초 미만이지만, 메모리 접근은 1회당 수십 나노초가 걸립니다. 따

> 그래서 등장한 것이 **캐시 메모리**입니다.&#x20;

캐시는 자주 쓰는 데이터를 메모리 대신 저장하고, 훨씬 더 빠르게 접근할 수 있게 도와주는 **중간 저장소**입니다.

<figure><img src="../../../.gitbook/assets/image (326).png" alt=""><figcaption></figcaption></figure>

### 3. 캐시 메모리의 동작 원리

#### 캐시 라인(Cache Line) 단위로 저장

캐시는 메모리 주소 1개만 가져오는 것이 아니라, `일정 크기의 데이터를 묶음(Cache Line)`으로 가져옵니다. 캐시 메모리에 캐시라인이라고 부르는 단위로 <mark style="color:red;">데이터를 읽어서 그 데이터를 레지스터로 옮깁니다.</mark> 예: 64Byte

#### 가상적인 CPU로 캐시 메모리를 살펴보자.

* 레지스터는 RO과 R1 두 종류이며, 둘 다 크기는 10바이트
* 캐시 메모리 크기는 50바이트
* 캐시라인 크기는 10바이트

1. CPU의 RO에 메모리 주소 300에 접근&#x20;

<figure><img src="../../../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>



2. CPU가 주소 300의 데이터를 다시 읽는다면, 예를 들어 RI으로 읽어 온다면 메모리에서 가져오   는 대신에 <mark style="color:red;">캐시 메모리에 곧바로 접근하면 되니까</mark> 빠르게 처리가 가능합니다.

<figure><img src="../../../.gitbook/assets/image (3).png" alt=""><figcaption></figcaption></figure>

3. RO 값을 변경하고 변경된 내용을 메모리 주소 300에 반영한다면 메모   리에 쓰기 전에 캐시 메모리에 먼저 저장합니다.&#x20;

이때 캐시라인에는 <kbd><mark style="background-color:blue;">메모리에서 읽어 들인 데이터가 변경되었다는 것을 뜻하는 표시<mark style="background-color:blue;"></kbd>를 붙입니다. 이런 표시가 붙은 캐시라인을 **더티하다**라고 이야기합니다

<figure><img src="../../../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

#### 더티 비트(Dirty Bit)

* CPU가 값을 바꾸면 **캐시에서 먼저 값이 바뀜**
* 나중에 이 변경된 값을 **메모리에 반영**할 때만 쓰기 작업 수행
* 이때 더티 비트를 체크해, 실제로 바뀐 것만 쓰도록 함

#### 메모리에 데이터를 쓰는 방법: 라이트 스루 vs 라이트 백

| 전략            | 설명                        | 장점        | 단점           |
| ------------- | ------------------------- | --------- | ------------ |
| Write-through | 캐시에 저장하면서 동시에 메모리에도 저장    | 데이터 유실 없음 | 느림           |
| Write-back    | 일단 캐시에만 저장하고, 나중에 메모리에 저장 | 빠름        | 전원 꺼지면 손실 위험 |

4. 더티 표시 붙은 캐시라인을 메모리에 반영하기

<figure><img src="../../../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

캐시 메모리가 가득 찼는데 **캐시에 존재하지 않는 데이터를 읽어 들이면** 기존의 캐시라인 중에서 하나를 버리고 **빈 캐시라인에 새로운 데이터를 넣습니다.**



5. 캐시 메모리가 가득 찬 상태일 때 캐시라인에서 데이터 버리기

<figure><img src="../../../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>

예시로주소&#x20;350의 데이터를 읽으면 캐시라인의 데이터 하나를 버리고(주소 340-350 필드)



6. 새로운 데이터를 캐시라인에 복사

<figure><img src="../../../.gitbook/assets/image (7).png" alt=""><figcaption></figcaption></figure>

지금 비운 캐시&#x20;라인에 읽어 올 주소의 데이터를 복사합니다.

이때 버리는 **캐시라인이 더티 상태**라면 메모리에 데이터를 저장하는 **클린 처리**를 하고 **버립니다.**

### 4. 캐시 교체 알고리즘과 스래싱

캐시 메모리는 용량이 작기 때문에, **자주 사용하는 데이터만 유지**하고 오래 안 쓰는 것은 버립니다. 이를 위한 알고리즘이 대표적으로 **LRU(Least Recently Used)** 등입니다.

하지만 어떤 프로그램이 너무 많은 메모리 범위를 널뛰듯 접근하면, 캐시가 **계속 버리고 읽는 상태**가 되면서 성능이 급감하는 **스래싱(thrashing)** 현상이 발생합니다.

### 5. 캐시의 작동원리 : 지역성(Locality)의 법칙

> "미리 쓸만한 데이터를 메인 메모리에서 캐시에 옮겨놓자"

캐시 메모리를 관통하는 핵심 개념은 데이터 지역성 (Locality) 입니다. 캐시 메모리는 이 원리로 작동합니다

1. **시간적 지역성(Temporal locality)**: 최근에 사용한 데이터는 가까운 미래에 또 사용된다\
   (예: 반복문 안 변수)
2. **공간적 지역성(Spatial locality)**: 한 번 접근한 메모리 근처가 다음에 또 사용된다\
   (예: 배열 순회)
3. **순차적 지역성( Sequential Locality)** : 비순차적 실행이 아닌 이상 명령어들이 메모리에 저장된 순서대로 실행되는 특성을 고려하여 다음 순서의 데이터가 곧 사용될 가능성이 높다.

예를 들어, 공간 지역성은 A\[0], A\[1] 로 구성되는 배열과 같이 참조된 데이터 근처의 데이터가 잠시후에 사용될 가능성이 높다는 것입니다.

반복문 돌릴 때 i, i+1, i+2 이런 식으로 접근하잖아요? 이걸 미리 예측해서 데이터를 캐시에 담아두는 겁니다.

결국 캐시는 이렇게 생각하면 돼요:

> "CPU가 가장 아끼는 친구! 항상 옆에 붙어 다니는 비서 같은 존재"

이 지역성을 활용하면 **적은 캐시 용량으로도 성능을 대폭 향상**시킬 수 있습니다.

### 6. 캐시 계층 구조 (L1 / L2 / L3)

#### L1 캐시

* CPU 내부에 존재
* 매우 빠르고 작음 (32KB \~ 64KB)
* 명령어와 데이터를 구분해 저장하는 경우도 있음

#### L2 캐시

* CPU 칩 내부 또는 외부
* 속도는 L1보다 느리지만, 용량이 큼 (256KB \~ 512KB)

#### L3 캐시

* 여러 코어가 공유하는 캐시
* 수 MB\~수십 MB 수준의 용량

### 7. 실험: 캐시 접근 속도 시각화

리눅스 환경에서 Go 언어와 Python을 이용해 다음 실험을 수행했습니다.

#### 실험 목적

* 캐시 메모리(L1/L2/L3)와 메인 메모리(RAM)의 **접근 속도 차이**를 측정합니다.

#### 실험 구조

1. 2ⁿ(KiB)의 크기를 가지는 버퍼를 반복 생성 (4KB \~ 64MB)
2. 각 버퍼의 모든 **캐시라인**에 접근
3. 접근 시간(나노초 단위)을 기록
4. `matplotlib`로 결과 그래프 작성

#### 실험 코드 (요약)

```go
// cache.go - Go로 메모리 접근 속도 측정
data, err := syscall.Mmap(-1, 0, bufSize, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_ANON|syscall.MAP_PRIVATE)
...
data[j] = 0  // 캐시라인 접근
...
```

```python
# plot-cache.py - 결과 시각화
ax.set_xlabel("[2^x KiB]")
ax.set_ylabel("접근 속도[접근횟수/나노초]")
```

### 8. 실험 결과: 접근 속도 차이

아래 그래프는 버퍼 크기에 따른 접근 속도 변화를 보여줍니다:

<figure><img src="../../../.gitbook/assets/image (327).png" alt=""><figcaption></figcaption></figure>

#### 전반적인결과 해석

* **L1, L2, L3 캐시의 경계에서** 속도가 계단식으로 변화합니다.
* 캐시보다 큰 버퍼를 접근할수록 속도가 **급격히 느려집니다. (L2→L3→RAM 이동)**
* 특정 용량(예: 32KB, 256KB, 4MB) 전후로 성능이 달라지는 이유는 해당 캐시 용량을 넘어서기 때문입니다.
* **작은 버퍼에서는 매우 빠름 (L1 캐시 내에서 동작)**

#### 그래프 구조 해석

* X축: 버퍼 크기 (2^x KiB 단위)
  * 2⁰ = 1KiB, 2¹ = 2KiB, ..., 2¹⁶ = 64MiB까지
  * 즉, <mark style="color:red;">버퍼 크기를 기하급수적으로 증가시키며 측정한 실험</mark>입니다.
* Y축: 접근 속도 (접근횟수/나노초)
  * 값이 **클수록 빠른 처리 속도**를 의미합니다.
  * 단위는 "얼마나 많은 캐시라인을 1나노초에 접근했는가"입니다.

#### 🔍 계단형 그래프가 의미하는 것

#### 1. **4\~64KiB 구간**

* 그래프는 1.4\~1.0 사이에서 **상대적으로 빠른 속도**를 유지합니다.
* → 이 구간은 `L1 캐시(보통 32~64KiB)`에 접근이 가능한 영역입니다.
* **시간적/공간적 지역성** 덕분에 대부분의 데이터가 L1에 캐시되어 있어 빠르게 처리됩니다.

#### 2. **64KiB\~512KiB 사이**

* 접근 속도가 급격히 떨어지기 시작합니다.
* → 이는 L1 캐시를 초과했기 때문이며, L2 캐시로 전환되었음을 나타냅니다.
* 속도는 떨어지지만 **L2는 L1보다는 느려도 RAM보다는 빠름**.

#### 3. **512KiB\~4MiB 구간**

* 두 번째 계단처럼 속도가 다시 하락 → L2 캐시도 초과됨.
* 이 구간은 **L3 캐시 또는 일부 RAM 접근이 시작되는 구간**입니다.

#### 4. **4MiB 이후 (대략 x=12 이상)**

* 접근 속도는 0.2\~0.4 수준으로 낮아집니다.
* 이 시점부터는 대부분의 접근이 \*\*메인 메모리(RAM)\*\*를 대상으로 이루어집니다.
* 캐시의 이점을 거의 누릴 수 없으며, 캐시 미스로 인한 성능 저하가 뚜렷해집니다.

#### 💡 왜 처음엔 속도가 점점 빨라지는가?

* 프로그램은 **단순히 메모리만 접근하는 것이 아니라**, 루프 변수 증가나 조건문(if) 처리 등의 **부가적인 명령어도 함께 실행**합니다.
* 버퍼가 작을 때는 부가 명령어의 실행 시간이 상대적으로 크게 작용하여, **접근 속도가 왜곡되어 더 낮게 나올 수 있음**.
* 버퍼가 커질수록 **순수 메모리 접근 시간이 지배적으로 작용**하게 되므로 측정이 더 정확해집니다.

#### 결론 요약

| 구간            | 캐시 계층 추정               | 해석                  |
| ------------- | ---------------------- | ------------------- |
| 4KiB\~64KiB   | **L1 캐시 영역**           | 가장 빠른 접근, 캐시 히트율 높음 |
| 64KiB\~512KiB | **L2 캐시 영역**           | 속도 감소 시작            |
| 512KiB\~4MiB  | **L3 캐시 또는 RAM 일부 접근** | 추가 감소               |
| 4MiB\~64MiB   | **메인 메모리(RAM)**        | 속도 최저, 캐시 미스 다수 발생  |

* **캐시 메모리의 효과는 명확히 존재**하며, 적절한 프로그램 구조는 성능을 비약적으로 향상시킬 수 있습니다.
* 특히 반복문 안에서 사용하는 배열은 **공간 지역성**을 최대한 활용해야 하며,
* 데이터 구조 설계 시 캐시 크기를 고려하는 습관이 중요합니다.

{% hint style="danger" %}
하지만 cache 프로그램 목적은 접근 속도의 절댓값을 구하는 것이 아니라 **접근할 메모리 영역&#x20; 크기 변화에 따른 메모리 접근 성능 변화를 확인**하는 데 있으므로 너무 깊이 생각하지 않아도&#x20;됩니다
{% endhint %}

> **“좋은 프로그램은 캐시를 이해하는 데서 시작된다.”**\
> – 컴퓨터 아키텍처 설계자들이 입을 모아 강조하는 말입니다

### 8. 메모리 접근 성능 실험(cache.go)

#### 실험 개요

* 점점 더 큰 버퍼(4KB\~64MB)를 만들고
* 각 캐시라인을 순차적으로 접근
* 접근 횟수 대비 소요 시간 측정

#### 결과

이 실험을 통해 **캐시의 존재와 효용성**을 수치로 증명할 수 있습니다.

### 9. 결론: 메모리 계층 구조의 존재 이유

왜 이렇게 복잡한 메모리 계층을 만들었을까요?

> "빠르고 비싼 기억장치는 작게, 느리고 저렴한 기억장치는 크게."

이 전략을 통해 우리는 컴퓨터 시스템에서

* **속도** (캐시)
* **유지 비용** (RAM/디스크)
* **유연성** (계층형 메모리 활용)\
  을 모두 확보할 수 있게 된 것입니다.

#### 캐시 최적화를 위해 할 수 있는 일

* 코드에서 **지역성(locality)** 을 높이도록 설계해야 합니다.
* 배열은 순차적으로 접근하고, 메모리 접근 패턴을 예측 가능하게 해야 합니다.
* 고성능 연산에서는 캐시라인을 고려한 **데이터 정렬(alignment)** 도 중요합니다.

## 🧠 리눅스 메모리 계층과 최적화 기술 요약

### 1. SMT(Simultaneous Multi Threading): CPU 유휴 자원 활용

#### ✔️ 개념 요약

* CPU 코어 내부의 **유휴 계산 자원**(정수 연산 유닛, 부동소수점 유닛 등)을 효율적으로 활용하기 위해,
* **하나의 CPU 코어가 여러 스레드를 동시에 실행**할 수 있도록 한 기술.
* SMT로 인해 **한 CPU 코어당 2개 이상의 논리 CPU**가 생김.

#### ✔️ 예시 상황

* 한 스레드가 메모리 접근을 기다릴 때, 다른 스레드가 대기 중인 계산 자원을 사용해 실행됨.
* 정수 연산만 사용하는 스레드와 부동소수점 연산만 사용하는 스레드가 SMT로 함께 실행되면 효과 극대화.

#### ✔️ SMT 성능 실험 결과

* 논리 CPU 수가 많아도, 실제 병목은 물리 코어 수(예: 4개)를 넘기면서부터 발생.
* SMT가 무조건 성능 향상으로 이어지진 않음 → 워크로드와의 **상성 중요**.

### 2. TLB(Translation Lookaside Buffer): 주소 변환 가속기

#### ✔️ 문제

* 프로세스는 **가상 주소**를 쓰지만, 실제 메모리는 **물리 주소**로 접근해야 함.
* 매번 **페이지 테이블**을 읽어야 하는데, 이 작업도 느림.

#### ✔️ 해결

* TLB는 **자주 참조하는 주소 변환 결과를 캐싱**.
* 메모리 접근 전에 먼저 TLB에서 변환 정보를 찾음 → 있으면 바로 사용(**TLB 히트**).
* 없으면 페이지 테이블을 접근해서 변환(**TLB 미스**), 이후 TLB에 등록.

### 3. 페이지 캐시(Page Cache): 파일 읽기/쓰기 가속

#### ✔ 역할

* CPU가 **파일에 접근할 때마다 디스크에 직접 접근하면 속도가 느림**.
* 리눅스는 파일 데이터를 메모리에 **캐싱**해서 속도 향상.
* 파일 읽기 = 페이지 캐시 확인 → 있으면 빠르게 메모리에서 제공.
* 파일 쓰기 = 일단 페이지 캐시에 저장(→ 더티 페이지로 표시), 나중에 디스크에 반영(→ 라이트백).

#### ✔️ 더티 페이지

* **디스크와 내용이 달라진 캐시 페이지**.
* 특정 시점에 디스크에 반영됨.
* 전원이 꺼지기 전에 기록되지 않으면 데이터 손실 가능성 → `O_SYNC` 플래그 사용 시 바로 기록 가능.

### 4. 페이지 캐시 효과 실험

#### ✔️ 실험 시나리오

* 1GiB 크기 파일 `testfile`을 만들고, 읽고 쓰는 시간을 측정.
* `oflag=sync` 사용 시: 디스크에 바로 기록 → 느림
* 일반 쓰기: 캐시에만 기록 → 빠름
* **읽기 테스트**: 페이지 캐시 삭제 전/후 속도 차이 확인

#### ✔️ 실험 결과 요약

| 실험 단계           | 결과 (대략적 속도)    |
| --------------- | -------------- |
| 디스크 동기화 쓰기      | 약 677MB/s (느림) |
| 일반 쓰기 (페이지 캐시만) | 약 1.5GB/s (빠름) |
| 디스크에서 읽기        | 약 1.8GB/s (처음) |
| 페이지 캐시에서 읽기     | 약 3.0GB/s (빠름) |

* 캐시 활용 여부에 따라 **속도 2\~3배 차이** 발생

### 추가 개념: `/proc/sys/vm/drop_caches`

* 페이지 캐시를 **강제로 삭제**해서 캐시 미적용 상태를 만드는 설정 파일.
* `echo 3 > /proc/sys/vm/drop_caches` 명령어로 실험 가능.
* 실제 운영 환경에서는 잘 사용하지 않지만, **성능 분석이나 테스트**에서는 유용.

### 정리 요약표

| 개념           | 역할/기능               | 특징                               |
| ------------ | ------------------- | -------------------------------- |
| SMT          | CPU 유휴 자원 활용        | 스레드 병렬 실행, 효율적일 수도 있고 역효과일 수도 있음 |
| TLB          | 가상 주소 → 물리 주소 변환 캐시 | 주소 변환 속도 향상, 페이지 미스 감소           |
| 페이지 캐시       | 파일 데이터를 메모리에 캐시     | 읽기/쓰기 가속, 더티 페이지 관리 필요           |
| O\_SYNC      | 파일 쓰기 시 디스크에도 즉시 반영 | 안정성 ↑, 속도 ↓                      |
| drop\_caches | 시스템 캐시 강제 제거        | 실험용, 운영 중 주의 필요                  |

## 비교

<figure><img src="../../../.gitbook/assets/image (328).png" alt=""><figcaption></figcaption></figure>
