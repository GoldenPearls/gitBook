---
icon: clock-ten-thirty
---

# 10장 가상화

## 1. 가상화 시스템의 구조 – 가짜가 진짜처럼 작동하는 원리

### 1) “한 기계 안에 또 다른 컴퓨터가 있다?” – 가상화란 무엇인가

요즘은 누구나 한 번쯤 이런 상황을 마주합니다:

* 윈도우에서 리눅스를 돌려본다든가,
* 개발 환경을 바꿔서 테스트하고 싶을 때,
* 클라우드에서 여러 운영체제를 동시에 운영할 때 등등.

이런 걸 가능하게 해주는 마법 같은 기능이 바로 **가상화(Virtualization)**&#xC785;니다.

가상화란 말 그대로, **물리적인 것이 아닌 ‘가짜’를 만들어 마치 진짜처럼 작동하게 만드는 기술**입니다.\
그런데 이 ‘가짜 컴퓨터’가 **진짜처럼 작동한다는 점**이 중요합니다.

{% hint style="success" %}
즉, 하나의 물리 기기에서 `여러 개의 가상 컴퓨터(VM, Virtual Machine)`가 동시에 실행될 수 있다는 뜻이죠.
{% endhint %}

가상화 기능은 PC나 서버 등의 물리적인 기기에서 가상 머신을 동작시키는 소프트웨어\
기능 및 그런 동작을 돕는 하드웨어 기능의 조합

### 2) 왜 가상화가 필요했을까?

가상 머신은 단순히 재미로 만든 게 아닙니다. 분명한 **목적과 필요**가 있었기 때문이죠:

#### 💡 가상 머신 활용 목적

1. **하드웨어 최대 활용**
   * 하나의 서버에서 여러 시스템 운영 → 자원 낭비 ↓
   * 머신 1대     에 가상 머신을 여러 개 만들고, 각각의 가상 머신을 고객이 빌려 쓰는 `IaaS(Infrastructure as a Service)`가 그 예
2. **서버 통합**
   * 수십 대의 서버 → 몇 대의 가상 머신으로 대체 가능
3. **레거시 시스템 수명 연장**
   * 오래된 OS도 새 기기에서 실행 가능 (지원 종료돼도 OK)
4. **크로스 운영체제 사용**
   * 윈도우 안에서 리눅스를, 리눅스 안에서 윈도우를 실행 가능
5. **개발 및 테스트 환경 분리**
   * 실환경을 흉내 낸 테스트 환경을 손쉽게 구성 가능
6. **자동화 테스트 및 배포**
   * 커널 개발, CI/CD 시스템에 활용

요즘 개발자들이 가상 머신을 많이 사용하는 이유도 여기에 있죠.

> "윈도는 써야 하지만, 리눅스 커널도 만져야 하고… 둘 다 필요해!"

### 3) 가상 머신의 구성 – 어떻게 만들어지나?

가상 머신을 가능하게 해주는 중심에는 바로 **가상화 소프트웨어(Virtualization Software)**&#xAC00; 있습니다.\
이 친구가 물리 기기의 자원을 잘게 나누고, 각각의 가상 머신에 배분해주는 역할을 하죠.

#### 📦 가상화 소프트웨어는 이렇게 생겼어요

<figure><img src="../../../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

가상 머신 입장에서는 **CPU, 메모리, 저장장치가 모두 자신만의 것처럼 보입니다.**\
하지만 실제로는 **물리 자원의 일부만을 쓰고 있는 것**이죠.

* 물리 CPU → PCPU (Physical CPU)
* 가상 CPU → VCPU (Virtual CPU)

{% hint style="danger" %}
"진짜 CPU는 하나지만, 가상 머신에는 각각 CPU가 할당된 것처럼 보인다!"
{% endhint %}

<figure><img src="../../../.gitbook/assets/image (3).png" alt=""><figcaption></figcaption></figure>

가상화 소프트웨어와 가상 머신의 관계는 커널의 프로세스 관리 시스템과 프로세스의 관계와\
무척 닮았습니다.

🖼️ 그림 설명: 가상화 소프트웨어 구조

**💡 전체 구조 요약**

```
[물리 기기] ──▶ [가상화 소프트웨어] ──▶ [가상 머신]
```

가상화 소프트웨어는 물리 기기의 자원을 일정 부분 나눠서 가상 머신에게 할당해주는 역할을 합니다.\
가상 머신 입장에선 자신만의 자원을 쓰는 것처럼 느끼게 되는 것이죠.

#### ✅ 구성 요소별 설명

| 항목                      | 설명                                                        |
| ----------------------- | --------------------------------------------------------- |
| **PCPU (Physical CPU)** | 물리 기기에 실제로 장착된 CPU. 가상 머신에서 요청하는 작업을 실행하는 실제 하드웨어         |
| **VCPU (Virtual CPU)**  | 가상 머신에 제공되는 CPU. 실제론 PCPU를 시간 단위로 쪼개어 배정한 것               |
| **메모리**                 | 물리 기기 전체 메모리 중 일부를 잘라서 가상 머신에 제공. 가상 머신은 자기 메모리라고 생각함     |
| **저장 장치**               | HDD, SSD 등의 저장 장치. 이 역시 일부만 가상 머신이 사용하게끔 나눠줌              |
| **가상화 소프트웨어**           | 이 모든 자원 분배를 제어하는 '매니저'. 사용자는 여러 VM을 만들고, 각각 원하는 만큼 자원을 배분 |

#### 🔁 작동 방식 요약

1. **물리 CPU(PCPU)**&#xB294; **가상 CPU(VCPU)**&#xC5D0;게 **일부 처리 능력**을 나눠줍니다.\
   → 여러 가상 머신이 같은 CPU를 시간 단위로 번갈아 사용해요.
2. **메모리**도 마찬가지로, 실제 메모리에서 일부 구간을 잘라서 가상 머신에 제공해요.\
   → 각 가상 머신은 자신이 ‘고유 메모리’를 가진 줄 압니다.
3. **저장 장치**도 파티션처럼 나눠서 가상 머신에 제공합니다.\
   → 이게 바로 가상 하드디스크(Virtual Disk, .vmdk 등)의 개념이에요.



**🧠 핵심 이해 포인트**

* **가상 머신은 실제 자원을 100% 가진 게 아니다!**\
  → 가상화 소프트웨어가 일부만 ‘할당’해서 진짜처럼 보이게 해주는 것일 뿐
* **가상 머신은 독립된 컴퓨터처럼 보이지만**,\
  실제로는 하나의 물리 기기 안에서 돌아가는 ‘논리적 컴퓨터’입니다.
* **가상화 소프트웨어는 운영체제의 커널처럼** 자원을 나눠 쓰게 해주는 관리자 역할을 합니다.

**🧩 비유로 쉽게 이해하기**

이 구조를 "공유 오피스"에 비유하면 이렇습니다:

| 항목        | 비유                                       |
| --------- | ---------------------------------------- |
| 물리 기기     | 공유 오피스 전체 건물                             |
| PCPU      | 건물 안에 있는 회의실 (한 개뿐)                      |
| VCPU      | 각 팀이 예약해서 쓰는 회의 시간 (10~~11시, 11~~12시...) |
| 메모리       | 각 팀에게 배정된 책상 수                           |
| 저장 장치     | 각 팀에게 배정된 사물함 공간                         |
| 가상 머신     | 독립된 스타트업 팀 (각자 자기 공간인 줄 알고 씀)            |
| 가상화 소프트웨어 | 공유 오피스 관리자, 공간과 자원을 배정해줌                 |

**🔚 요약**

* 그림 10-02는 가상화 소프트웨어가 **물리 자원을 가상 머신에 할당하는 구조**를 시각화한 것입니다.
* 이 구조 덕분에 우리는 하나의 물리 컴퓨터에서 여러 개의 운영체제와 프로그램을 동시에 실행할 수 있습니다.
* 실제 자원은 나누어 쓰는 것이며, 가상 머신은 '진짜처럼 착각하도록' 정교하게 설계된 가짜 컴퓨터입니다.

### 4) 가상 머신은 어떻게 OS를 실행하는가?

우리가 컴퓨터에 리눅스를 설치하는 구조와 거의 같습니다.

#### 💻 일반적인 구조 - 물리 기기에 os를 설치한 경우

<figure><img src="../../../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

#### 🧩 가상 머신 위의 구조는? -  가상 머신에OS 설치한 경우

<figure><img src="../../../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>

이처럼 가상화 소프트웨어 위에 \[그림 10-03]에서 봤던 시스템이 올라가는 구조가 됩니다. 가상 머신과 물리 기기는 장치 구성 등 일부를 제외하면 차이가 없으므로 **가상 머신이 제공하는 각종 하드웨어를 지원하는 OS가 있으면 뭐든지 설치할 수 있습니다.**

즉, 각 가상 머신 위에 원하는 **게스트 OS**를 설치하고, 그 안에서 진짜 컴퓨터처럼 동작하게 하는 겁니다.

**가상 소프프트웨어를 구현하는 방법**

* 하드웨어에 직접 하이퍼바이저  &#x20; hypervisor 가상화 소프트웨어를 설치하는 방법
* 기존 OS를 바탕으로 애플리케이션으로 동작하는  &#x20;방법
* 유명한 소프트웨어
  * VM웨어VMware 사의 각종 제품
  * 오라클Oracle 사의 버추얼박스VirtualBox
  * 마이크로소프트 Microsoft 사의 하이퍼-VHyper-V
  * 시트릭스 시스템즈Citrix Systems 사의 젠x

### 5) 가상화 소프트웨어의 3종 세트

이번 장에서는 **리눅스 기반의 오픈소스 가상화 환경**을 다루고 있습니다. 대표 구성은 이렇습니다:

| 소프트웨어            | 역할                              |
| ---------------- | ------------------------------- |
| **KVM**          | 리눅스 커널에 통합된 가상화 기능              |
| **QEMU**         | 하드웨어(특히 CPU) 에뮬레이터 역할           |
| **virt-manager** | GUI로 가상 머신 생성, 삭제, 조작 (QEMU 관리) |

이 조합은 실제로 기업 환경에서도 많이 쓰이며, 완전히 무료로 사용 가능합니다.

> 리눅스에선 `sudo apt install virt-manager` 한 줄이면 세팅 가능!

### 6) 가상 머신은 프로세스다? – 리눅스 커널이 바라보는 가상 머신

QEMU나 virt-manager는 결국 **리눅스 프로세스**입니다.\
즉, **리눅스 입장에서는 가상 머신도 그냥 실행 중인 프로그램 하나**인 거예요.

* `ps aux`로 보면 QEMU가 떡하니 떠 있음
* `top`으로 보면 CPU 자원도 쓰고 있음

그러면 궁금해지죠?

> “이 QEMU가 어떻게 리눅스, 윈도 같은 OS를 실행할 수 있는 걸까?”

그건 바로 **KVM의 도움** 덕분입니다.

* QEMU는 하드웨어를 흉내 내고,
* KVM은 CPU 실행을 하드웨어 수준에서 빠르게 처리합니다.

#### 💡 요약

* virt-manager: 가상 머신 정의 및 관리
* QEMU: 실행자
* KVM: 성능 최적화를 위한 가속기

### 7) 호스트 OS와 게스트 OS

<figure><img src="../../../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

| 용어         | 의미                           |
| ---------- | ---------------------------- |
| **호스트 OS** | 가상화 소프트웨어가 설치된 OS (실제 리눅스 등) |
| **게스트 OS** | 가상 머신 안에 설치된 운영체제            |

<mark style="color:red;">게스트 OS는 자신이 가상 머신 위에 있다는 걸</mark> **몰라도 됩니다.**\
자신만의 CPU와 메모리를 가지고 있다고 믿죠.

그런데 실제로는…

* CPU는 나눠쓴다 (스케줄링)
* 메모리도 나눠쓴다
* 입출력도 가상 장치를 통해 공유된다

> 즉, "분리된 듯 연결된 상태"에서 각 가상 머신이 돌아가는 거죠.

### 8) virt-manager는 ‘가짜 손’이다

virt-manager는 마치 **가상의 손처럼** 행동합니다.

* 가상 머신을 **켜고 끄고**,
* 마우스와 키보드 입력을 **전달하고**,
* CD/DVD 장치를 **마운트하거나 꺼내는** 일도 할 수 있어요.

즉, **실제 사람이 컴퓨터를 조작하는 것처럼**\
→ virt-manager가 그걸 대신해주는 거예요.

{% hint style="success" %}
virt-manager와 QEMU는 리눅스 커널 입장에서는 그저 **프로세스에 불과**합니다. 따라서 가상\
화 소프트웨어와 함께 일반적인 프로세스를 실행합니다.
{% endhint %}

```
┌──────────────┐
│ virt-manager  │ ← 마우스/키보드, 화면 출력 연결
│              │
│    QEMU      │ ← 가상 머신 실제 실행
└──────────────┘
```

#### 🔁 전체 흐름 순서

<figure><img src="../../../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>

1. **virt-manager가 가상 머신 설계도 작성**

* 사용자에게 GUI를 제공하여 가상 머신의 사양을 설정함
  * CPU 개수
  * 메모리 용량
  * 디스크 이미지(.qcow2)
  * 네트워크 구성
  * ISO 이미지 삽입 등



2. &#x20;**virt-manager가 QEMU 프로세스를 기동**

* 사용자가 '가상 머신 시작' 버튼을 누르면,
* QEMU가 백그라운드에서 새로운 가상 머신 인스턴스를 실행함



3. &#x20;**QEMU와 KVM이 함께 가상 머신을 실행**

* **QEMU**: CPU 에뮬레이션, 하드웨어 장치 흉내
* **KVM**: CPU 가상화를 리눅스 커널 수준에서 직접 실행 (속도 빠름)



4. &#x20;**virt-manager가 전원 관리 및 삭제 기능 수행**

* 사용자가 끄기/재시작/삭제 버튼을 누르면 QEMU와 커널에 요청을 전달

#### 🧠 각 구성 요소의 역할

| 구성 요소            | 설명                                            |
| ---------------- | --------------------------------------------- |
| **virt-manager** | 사용자가 가상 머신을 쉽게 만들고 관리하도록 도와주는 GUI 툴           |
| **QEMU**         | CPU, I/O, 네트워크 등 가상 하드웨어를 '에뮬레이션'해주는 프로그램     |
| **KVM**          | 리눅스 커널 안에서 동작하는 고속 가상화 엔진 (QEMU와 연결해서 성능을 높임) |

> 💡 virt-manager는 그냥 GUI일 뿐, 실제 실행은 QEMU/KVM이 합니다.

그림 10-06은 다음과 같은 구조를 보여줍니다:

<figure><img src="../../../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>

#### 🔧 virt-manager의 주요 기능 요약

| 기능           | 설명                                       |
| ------------ | ---------------------------------------- |
| 디스플레이 출력 보기  | 가상 머신 내부 화면을 GUI로 보여줌                    |
| 키보드/마우스 조작   | 마우스, 키보드 이벤트를 가상 머신으로 전달                 |
| 전원 켜기/끄기/재시작 | 가상 머신의 전원 상태 관리                          |
| 장치 추가/삭제     | 디스크, 네트워크, USB 등 장치를 가상 머신에 동적으로 붙이거나 제거 |
| ISO 삽입/제거    | 가상 CD/DVD 드라이브에 이미지 삽입 또는 추출             |

#### 📌 핵심 요약

* **virt-manager**는 사용자와 QEMU/KVM 사이의 ‘조정자’입니다.
* 실제 가상 머신은 **QEMU + KVM** 조합으로 실행됩니다.
* 이 조합은 **직접 명령어로 조작할 수도 있고**, **virt-manager로 간편하게 제어할 수도 있어요**.
* 모든 동작은 결국 리눅스에서 돌아가는 **하나의 프로세스처럼 취급됩니다**.



### 9) Nested Virtualization – 가상 머신 안의 가상 머신

> "가상 머신 안에서도 가상 머신을 만들 수 있을까?"

네, 요즘은 가능합니다! 이걸 `중첩 가상화(nested virtualization)`라고 합니다.

특히 **클라우드 환경에서 많이 쓰입니다.**

* 예: Google Cloud 위에 리눅스 가상 머신 생성
* 그 리눅스 가상 머신에서 또 다른 리눅스/윈도 설치

하지만…

* 하이퍼바이저(가상화 엔진)가 중첩 가상화를 **지원해야** 합니다
* 성능도 다소 떨어질 수 있습니다

> “클라우드 속 가상 머신 안에서 또 다른 클라우드를 띄운다”\
> 요즘 DevOps 환경에서 테스트나 CI/CD용으로 자주 사용됩니다.

### ✅ 마무리: 가상은 진짜가 될 수 있다

가상 머신은 이제 단순한 실험 도구를 넘어, **진짜 운영 환경의 핵심 인프라**로 자리잡았습니다.

* 클라우드? = 가상 머신의 제국
* 서버 통합? = 가상화 없인 불가능
* 테스트 환경 구축? = 가상 머신이 없으면 너무 불편

이 모든 걸 가능하게 하는 건, OS와 커널이 만들어내는 **철저한 격리와 가상화 기술** 덕분입니다.

> 진짜처럼 보이는 가짜,\
> 그러나 실제보다 더 유용한 그 무엇.

이것이 가상화의 매력이자, 우리가 반드시 알아야 할 시스템의 세계입니다.

## 2. CPU의 가상화 기능과 QEMU+KVM의 동작 구조 정리

### 1) 사용자 모드와 커널 모드의 CPU 동작

<figure><img src="../../../.gitbook/assets/image (343).png" alt=""><figcaption></figcaption></figure>

| 구분         | 설명                                        |
| ---------- | ----------------------------------------- |
| **사용자 모드** | 일반 애플리케이션이 실행되는 영역→ 제한된 자원 접근만 가능         |
| **커널 모드**  | 시스템 콜, 드라이버 등 OS 커널이 실행되는 영역→ 모든 자원 접근 가능 |
| **전환 조건**  | 시스템 콜, 인터럽트 등 발생 시 사용자 → 커널 모드로 진입하고 복귀함  |

> → CPU는 보안과 안정성을 위해 **사용자/커널 모드를 구분**해서 자원 접근을 제한합니다.

### 2) 가상화 확장: VMX-root vs VMX-nonroot

<figure><img src="../../../.gitbook/assets/image (344).png" alt=""><figcaption></figcaption></figure>

가상화를 지원하는 CPU는 **기존 사용자/커널 모드 구조를 확장**하여 다음과 같은 상태를 가집니다.

| 구분                 | 설명                                                                                         |
| ------------------ | ------------------------------------------------------------------------------------------ |
| **VMX-root 모드**    | <p><strong>하이퍼바이저(KVM)</strong>나 <strong>호스트 커널</strong>이 동작하는 영역<br>물리 기기 처리를 하는 모드 </p> |
| **VMX-nonroot 모드** | <p>가상 머신(게스트 OS)이 실행되는 상태게스트 입장에서 '자기 OS의 사용자/커널 모드'가 있음<br>가상 머신 처리 모드</p>                |
| **전환 조건**          | 게스트 OS가 하드웨어 접근 또는 특권 명령 실행 시 VMX-root로 전환됨 (트랩)                                           |

<figure><img src="../../../.gitbook/assets/image (345).png" alt=""><figcaption></figcaption></figure>

이 구조 덕분에 CPU는 **가상 머신을 하드웨어 수준에서 분리하고 제어**할 수 있습니다.

### 3) CPU 가상화 기술: VT-x와 SVM

<figure><img src="../../../.gitbook/assets/image (346).png" alt=""><figcaption></figcaption></figure>

| CPU 제조사 | 가상화 명칭                           |
| ------- | -------------------------------- |
| Intel   | **VT-x** (Intel Virtualization)  |
| AMD     | **SVM** (Secure Virtual Machine) |

* 기능은 유사하지만 **명령어셋이 다름**
* 이렇게 아키텍처별로 달라지는 부분은 KVM이 해결  해주는데, 커널에서는 이를 **KVM이 추상화**해 처리함

**VT-X 또는 SVM이 활성 여부 확인 명령어**

```bash
egrep -c '^flags.*(vmx|svm)' /proc/cpuinfo
```

> 출력이 1 이상이면 활성화 상태.\
> 만약 0이라면 BIOS에서 해당 기능이 꺼져 있을 가능성이 있음.

### 4) QEMU + KVM 가상 머신의 동작 구조

#### 1. 물리 환경에서 장치 접근 흐름

<figure><img src="../../../.gitbook/assets/image (347).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../../../.gitbook/assets/image (348).png" alt=""><figcaption></figcaption></figure>

#### 2. 가상 환경에서의 흐름 (QEMU + KVM)

<figure><img src="../../../.gitbook/assets/image (349).png" alt=""><figcaption></figcaption></figure>

가상 머신에서 보면 \[그림 10-11]의 물리 기기와 똑같은 일을 하는 것처럼 보이지만(처리 1.\
2, 7, 8) 무대 뒤를 보면 <mark style="color:red;">물리 기기에서 QEMU와 KVM이 하드웨어를 에뮬레이션하고 있습</mark>\ <mark style="color:red;">니다(처리 3-6).</mark>&#x20;

{% hint style="danger" %}
**QEMU**: 가상 하드웨어를 에뮬레이션하는 사용자 공간 프로그램\
**KVM**: CPU 가상화, 메모리 보호 등을 커널에서 담당
{% endhint %}

#### 🖼️ \[그림 10-12] 가상 머신의 장치 접근 흐름

1️⃣ **가상 머신 내 프로세스가 시스템 콜 실행**\
→ 파일이나 디바이스에 접근하기 위해 시스템 콜을 호출합니다.\
→ 게스트 리눅스 커널로 진입 (게스트 OS의 커널 모드)

2️⃣ **게스트 커널이 장치에 접근하려고 함**\
→ 하지만 이 장치는 실제 장치가 아니라 QEMU가 에뮬레이션하는 장치이므로,\
**VM Exit** 발생 (즉, CPU가 하드웨어 자원을 직접 사용할 수 없다고 판단)

3️⃣ **KVM이 VM Exit을 감지하고 모드를 VMX-root로 전환**\
→ KVM은 이 이벤트를 QEMU에게 전달\
→ QEMU는 사용자 공간에서 장치 접근을 에뮬레이션 처리

4️⃣ **QEMU가 처리한 결과를 KVM으로 다시 전달하고 복귀**

#### \[그림 10-13] 위 흐름에 따른 CPU의 모드 전환 (시간 흐름)

#### 1. 가상 머신 입장에서 본 CPU 상태 변화

```
시간축 →
┌──────────────────────────────────────────────┐
│ 가상 머신에서 CPU의 상태: 사용자 모드 → 커널 모드 → 복귀 │
└──────────────────────────────────────────────┘

- 가상 머신 내부에선 평범한 커널 모드 진입처럼 보임
```

#### 2. 물리 기기 입장에서 본 실제 CPU 상태 변화

```
시간축 →
┌──────────────────────────────────────────────┐
│ VMX-nonroot 사용자 모드                        │
│    ↓                                            │
│ VMX-nonroot 커널 모드   ← (게스트 커널 진입)      │
│    ↓                                            │
│ VM Exit → VMX-root 모드로 전환 (KVM으로 전환됨)   │
│    ↓                                            │
│ QEMU (유저 공간)에서 장치 처리                  │
│    ↓                                            │
│ 복귀 후 다시 VMX-nonroot → 게스트 OS로 복귀     │
└──────────────────────────────────────────────┘
```

#### 핵심 요약

| 단계   | 설명                 | 모드 전환                |
| ---- | ------------------ | -------------------- |
| \[1] | 게스트 사용자 모드         | VMX-nonroot 사용자      |
| \[2] | 게스트 커널 모드          | VMX-nonroot 커널       |
| \[3] | 장치 접근 → VM Exit 발생 | VMX-root 커널 (KVM 진입) |
| \[4] | QEMU 호출 → 에뮬레이션    | 사용자 공간 처리            |
| \[5] | 완료 후 복귀            | 다시 VMX-nonroot 커널    |
| \[6] | 게스트 사용자 모드로 복귀     | VMX-nonroot 사용자      |

#### 💡 결론 요약

* **게스트 OS 내부에서는** 그냥 커널이 장치를 접근하는 평범한 시스템 콜처럼 보입니다.
* **하지만 물리 CPU 입장에서 보면**\
  VMX-nonroot → VMX-root → 유저 공간(QEMU) → 복귀 과정을 반복하며 매우 복잡하게 작동합니다.

{% hint style="danger" %}
이 구조 덕분에 가상 머신은 실제 하드웨어처럼 보이면서도, 안전하게 격리되어 실행됩니다
{% endhint %}

#### CPU 가상화 기능이 없던 시절의 가상화는 어떻게 가능했을까?

#### ✅ 배경

오늘날은 대부분의 CPU가 **하드웨어 수준의 가상화 기능(VT-x for Intel, SVM for AMD)**&#xC744; 내장하고 있지만,\
**초창기에는 이런 기능 없이도 가상화 소프트웨어(VMware, VirtualBox 등)**&#xAC00; 존재했습니다.

#### 🔍 그런데 문제는?

가상 머신이 동작하기 위해서는 내부에서 `하드웨어 자원(예: 장치, 레지스터 등)`에 접근하려 할 때\
**이를 호스트(물리 기기)에서 감지하고 제어를 넘겨받아야** 합니다.

> 예: 게스트 OS가 `IN`이나 `OUT` 명령으로 I/O 장치를 직접 조작하려 할 때,\
> 그걸 **호스트가 가로채지 못하면 위험하거나 충돌**이 일어날 수 있음.

#### 🛠️ 해결책 – **반가상화 (Para-Virtualization)**

가상화 기능이 없던 시절에는 이런 문제를 소프트웨어적으로 해결했는데, 그 방식이 바로 **반가상화**입니다.

**📌 반가상화란?**

* 게스트 운영체제(커널 등)의 일부 코드를 **수정**해서,
* **하드웨어 자원에 직접 접근하지 않고**,\
  `특수한 API 또는 하이퍼바이저 호출(hypercall)`을 통해 대신 처리하게 만듭니다.

**🧠 예시**

> 원래 `mov` 명령으로 특정 레지스터에 접근할 게스트 커널이 있었다면,\
> 반가상화에서는 그 부분을 `hypercall()` 함수로 교체해 **호스트가 직접 개입**할 수 있게 합니다.

#### ⚠️ 단점

* 게스트 OS의 커널 코드를 **수정**해야 하기 때문에,
* **운영체제 수정이 가능하거나 오픈소스**일 때만 사용 가능함 (예: 리눅스).
* 윈도우 같은 **폐쇄형 OS에서는 어려움**이 있음.

#### 🔍 요약 표

| 구분        | 하드웨어 가상화 있음 (VT-x 등) | 하드웨어 가상화 없음 (초기)                    |
| --------- | -------------------- | ----------------------------------- |
| 방식        | CPU가 직접 VM을 지원       | 소프트웨어가 직접 제어 (반가상화)                 |
| 게스트 커널 수정 | ❌ 불필요                | ✅ 필요                                |
| 예시 기술     | KVM, Hyper-V         | Xen (early version), VMware (early) |
| 감지 방법     | VM Exit 발생 시 자동 전환   | 실행 코드를 미리 바꿔서 호스트가 감지               |

#### 📚 더 알아보고 싶다면?

* 검색 키워드: `Para-virtualization`, `Xen hypercall`, `VMware binary translation`

### 5) 가상 머신은 프로세스다?

가상 머신을 실행하면 **`qemu-system-x86_64`라는 하나의 프로세스**로 리눅스 호스트에 나타납니다.

```bash
$ ps ax | grep qemu-system
```

| 요소                   | 의미                           |
| -------------------- | ---------------------------- |
| `qemu-system-x86_64` | 가상 머신 실행 프로세스                |
| `virt-manager`       | GUI 기반 가상 머신 관리 도구           |
| `virsh`              | CUI 기반 가상 머신 제어 도구 (libvirt) |

📝 `virsh list`로 현재 실행 중인 가상 머신을 확인할 수 있고,\
`virsh dumpxml [vmname]`으로 설정(XML)을 확인할 수 있습니다.

### 6) 가상 머신을 만드는 명령어 예시

```bash
virt-install --name ubuntu2004 \
  --vcpus 1 --cpuset=0 \
  --memory 8192 --os-variant ubuntu20.04 \
  --graphics none --extra-args 'console=ttyS0'
```

이 명령은 다음 설정을 가진 가상 머신을 생성합니다:

* 이름: ubuntu2004
* CPU 1개
* 메모리 8GB
* 설치용 ISO를 통해 부팅

### 7) 가상 머신 1개 = qemu 프로세스 1개

2개의 가상 머신을 실행하면 다음과 같이 **2개의 qemu 프로세스가 생깁니다**.

```bash
$ ps ax | grep qemu-system
21945 ? Sl0:09 /usr/bin/qemu-system-x86_64 -name guest=ubuntu2004
22004 ? Sl0:07 /usr/bin/qemu-system-x86_64 -name guest=ubuntu2004-clone
```

### 8.) IaaS & 오토 스케일링 구조

가상화가 중요한 이유 중 하나는 **클라우드 자동화 및 스케일링에 필수**이기 때문입니다.

| 단계           | 설명                         |
| ------------ | -------------------------- |
| 1️⃣ 부하 감지    | 모니터링 시스템이 부하 상승 감지         |
| 2️⃣ 가상 머신 조작 | 관리 도구(libvirt 등)를 통해 자동 생성 |
| 3️⃣ 서비스 확장   | 자동으로 가상 머신 추가 후 트래픽 분산     |

📌 클라우드 시스템에서는 사람이 수동으로 가상 머신을 관리하지 않고,\
**프로그래밍 방식으로 VM을 조작**할 수 있어야 합니다.

### 📌 핵심 요약

| 구분              | 설명                                   |
| --------------- | ------------------------------------ |
| **VMX-root**    | 호스트 OS, 하이퍼바이저가 동작                   |
| **VMX-nonroot** | 게스트 OS(가상 머신)가 실행되는 모드               |
| **KVM**         | 커널 레벨 가상화 처리 담당                      |
| **QEMU**        | 사용자 공간의 가상 장치 에뮬레이션 담당               |
| **libvirt**     | 가상 머신 제어 라이브러리 (virsh, virt-manager) |
| **IaaS 오토스케일링** | 가상 머신을 자동으로 증설/축소할 수 있는 구조           |

## 3. 가상 머신은 호스트 OS에서 어떻게 보일까?

### 1) 가상 머신의 구성과 작성 방법

가상 머신을 만들기 위해선 다음과 같은 구성 요소를 정의합니다:

| 항목     | 설명                         |
| ------ | -------------------------- |
| OS     | 우분투 20.04 / x86\_64        |
| 가상 CPU | 1개 (vCPU), PCPU 0번에 pin 지정 |
| 메모리    | 8GiB                       |
| 디스크    | 기본 드라이버인 virtio 사용         |

#### 🛠️ virt-manager 또는 CLI로 작성 가능

```bash
virt-install --name ubuntu2004 \
             --vcpus 1 --cpuset=0 \
             --memory 8192 \
             --os-variant ubuntu20.04 \
             --graphics none \
             --extra-args 'console=ttyS0'
```

* 위 명령은 GUI 없이 CLI에서 가상 머신을 만들기 위한 예시입니다.
* `console=ttyS0`은 설치 진행 상황을 콘솔에서 확인하기 위한 옵션입니다.

### 2) 가상 머신 설정(XML)의 정체

virt-manager 또는 virsh는 **XML 형식의 설정 파일**을 기반으로 가상 머신을 구성합니다.

#### 예시: virsh dumpxml ubuntu2004

```xml
<domain type='kvm' id='23'>
  <name>ubuntu2004</name>
  <memory unit='KiB'>8388608</memory>
  <vcpu placement='static' cpuset='0'>1</vcpu>
  <devices>
    <disk type='file' device='disk'>
      <source file='/var/lib/libvirt/images/ubuntu2004.qcow2'/>
    </disk>
  </devices>
</domain>
```

#### 주요 필드 요약

<table><thead><tr><th width="350">항목</th><th>값</th><th>설명</th></tr></thead><tbody><tr><td><code>&#x3C;name></code></td><td>ubuntu200</td><td>가상 머신 이름 (식별자)</td></tr><tr><td><code>&#x3C;memory></code></td><td>8388608(8GIB</td><td>메모리 용량 (KiB 단위)</td></tr><tr><td><code>&#x3C;vcpu></code></td><td>1</td><td>vCPU 개수 및 CPU 핀 설정<br>cpuset attribute 값은 VCPU가 <strong>동작 가능한 VCPU 목록</strong></td></tr><tr><td><code>devices</code></td><td>ㅡ</td><td>가상 머신에 설치된 하드웨어 목</td></tr><tr><td><code>&#x3C;disk></code></td><td>ㅡ</td><td>저장 장치. 이 뒤에 있는 file은 저장 장치에 대응하는 파일</td></tr></tbody></table>

이 XML은 내부적으로 `qemu-system-x86_64` 명령어로 변환되어 실행됩니다.

### 3) 실행 후 호스트에서의 가상 머신 인식

#### 가상 머신 실행

```bash
virsh start ubuntu2004
```

#### 가상 머신 확인

```bash
virsh list
```

출력 예:

```
 Id   Name        State
-----------------------------
 23   ubuntu2004  running
```

#### ps 명령으로 확인

```bash
ps ax | grep qemu-system
```

```
19904 ? Sl   3:06 /usr/bin/qemu-system-x86_64 -name guest=ubuntu2004 ...
```

✔ **포인트**: `qemu-system-x86_64` 프로세스 하나가 가상 머신 하나에 대응합니다.

> 가상 머신은 qemu-system-x86\_64 프로세스에 1대1로 대응합니다.

### 4) XML ↔ qemu 명령어 매핑 구조

{% hint style="warning" %}
프로세스에는 수많은 명령줄 인수가 지정됩니다. 그중에서 비교적 알아보기 쉽고 중요해 보이는 인수에 주목하면 **cpu device, drive처럼 하드웨어**와&#x20;관련된 내용이 많다는 것을 알 수 있습니다. 게다가 이런 값은 앞에서 본 XML 파일 내용과 상당히 비슷합니다. 그건 <mark style="color:red;">virsh가 가상 머신의 XML 파일 내용을 qemu-system-x86\_64 명령어가 해석할 수 있는 형태로 변환해서 인수로 전달하기 때문</mark>입니다(그림 10-14).
{% endhint %}

<figure><img src="../../../.gitbook/assets/image (352).png" alt=""><figcaption></figcaption></figure>

* virsh 또는 virt-manager → 내부적으로 XML 구성
* libvirt → 이를 qemu 명령어로 변환
* qemu-system-x86\_64 → 실제 가상 머신 실행 프로세스

```bash
# 예시 명령어의 일부
/usr/bin/qemu-system-x86_64 \
  -name guest=ubuntu2004 \
  -smp 1 \
  -m 8192 \
  -drive file=ubuntu2004.qcow2 ...
```

이렇게 **가상 머신 1개 = qemu 프로세스 1개**로 확인할 수 있습니다.

### 5) 여러 가상 머신을 실행하면?

<figure><img src="../../../.gitbook/assets/image (351).png" alt=""><figcaption></figcaption></figure>

#### 복제 예시 (virt-clone 사용)

```bash
virt-clone -o ubuntu2004 -n ubuntu2004-clone --auto-clone
```

#### 두 가상 머신 실행 후 확인

```bash
virsh start ubuntu2004
virsh start ubuntu2004-clone
ps ax | grep qemu-system
```

출력 예:

```
21945 ? Sl  0:09 /usr/bin/qemu-system-x86_64 -name guest=ubuntu2004 ...
22004 ? Sl  0:07 /usr/bin/qemu-system-x86_64 -name guest=ubuntu2004-clone ...
```

* **2개의 가상 머신 = 2개의 qemu 프로세스**
* 각각 독립된 가상 하드웨어, 메모리, CPU를 갖고 동작

### 6) IaaS 환경의 오토 스케일 구조와 libvirt

#### 오토 스케일은 어떻게 동작할까?

**IaaS(Infra as a Service)** 환경에서는 <mark style="color:red;">시스템 부하에 따라 가상 머신을 자동으로 늘리거나 줄입니다.</mark>

* 부하 감지 → 스케일링 컨트롤러가 판단
* libvirt나 Hypervisor API를 통해 가상 머신 조작
* 프로그램이 직접 가상 머신 생성을 명령

#### 구조 흐름 요약

<figure><img src="../../../.gitbook/assets/image (350).png" alt=""><figcaption></figcaption></figure>

```
[서비스] → 부하 감지 → [오토 스케일 시스템] → libvirt 호출 → 가상 머신 생성
```

이 방식으로 **사람의 개입 없이도** 자동으로 인프라를 구성할 수 있습니다.&#x20;

> laas 환경에서는 시스템 부하에 따라 시스템에 만들 가상 머신 수를 조절하는 `오토 스케일(auto scale)` 기> 능이 있습니다.&#x20;

IaaS 서비스 제공자나 시스템 관리자가 직접 가상 머신을 조작하는 게 아니라 \[그림 10-16]처럼 **시스템 부하에 변화가 생기면 프로그램에서 가상 머신을 높이거나 줄이는 방식으로 동작**합니다.

### ✅ 요약 정리

| 개념       | 내용                                  |
| -------- | ----------------------------------- |
| 가상 머신 실행 | qemu-system-x86\_64 프로세스로 나타남       |
| virsh    | 가상 머신을 CLI로 조작하는 도구 (libvirt 기반)    |
| XML 설정   | 가상 머신 하드웨어 및 메모리 설정을 정의             |
| 여러 가상 머신 | 각각 독립적인 qemu 프로세스로 운영됨              |
| 오토 스케일링  | 부하에 따라 자동으로 가상 머신 수 조절 가능 (IaaS 구조) |

## 4. 가상화 환경과 프로세스 스케줄링 완전 정리

### 1) 가상 머신 위에서의 `sched.py` 동작

<figure><img src="../../../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

#### 실험 시나리오

* 가상 머신에서 `sched.py`를 병렬도 2로 실행.
* 각 프로세스는 100ms 동안 CPU를 사용하며, 교대로 실행됨.

#### 실험 결과 요약

* 두 프로세스가 **교대로 동작**함.
* 실제 동작은 가상 머신의 `VCPU0`에서 돌아가며, 이는 호스트의 `qemu-system-x86`의 스레드로 존재.

{% hint style="danger" %}
적어도 **VCPU마다 하나 이상의 스레드가&#x20;있다는 점**만 이해
{% endhint %}

#### 동작 흐름 구조

| 계층         | 동작                                                        |
| ---------- | --------------------------------------------------------- |
| **게스트 OS** | `VCPU0`에서 `sched.py` 실행                                   |
| **호스트 OS** | `qemu-system-x86` 프로세스 내 `VCPU0` 스레드가 물리 CPU(PCPU0)에서 실행됨 |

### 2) `inf-loop.py`를 함께 돌리면?

#### 시나리오

* 호스트의 PCPU0에서 `inf-loop.py`를 강제로 실행 (`taskset -c 0`)
* 동시에 가상 머신 내부에서 `sched-virt.py 2` 실행

#### 전제 구조

* 가상 머신의 VCPU0은 호스트 OS에서 **qemu-system-x86\_64 프로세스의 스레드**로 실행됩니다.
* 이 VCPU0 스레드는 물리 CPU인 **PCPU0 위에서만 동작하도록 핀 지정**되어 있습니다.
* 그런데 이 PCPU0에서 **다른 프로세스(예: inf-loop.py 같은 CPU를 100% 쓰는 프로그램**)가 같이 동작하면 어떻게 될까요?

#### 결과 요약

* 프로세스 0과 1 모두 일정 시간 멈춰 있는 구간이 생김
* 이는 **호스트의 `inf-loop.py`가 VCPU 스레드를 밀어낸 결과**

#### 실행 흐름

<figure><img src="../../../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

PCPUO에서는 **VCPUO 스레드 외에는 동작하는 게 없습니다.**

### 3) 통계 정보 분석

> 그러면> &#x20;PCPUO에서 VCPUO 스레드 이외의 처리가 동작한다면 어떻게 되는지 살펴봅시다

#### (1) **VCPU만 실행 중**인 상태

* 물리 장비 sar

```bash
$ sar -P 0 1
CPU %user %nice %system %iowait %steal %idle
0   100    0      0        0        0        0
```

* top 명령어

```bash
qemu-system-x86  → 100%
```

* 가상 머신 내부 sar

```bash
CPU %user %system %steal
0   100    0       0
```

✅ **VCPU가 PCPU를 독점**하는 상황. `%steal`은 없음.

#### (2) **VCPU와 inf-loop.py 둘 다 실행 중**인 상태

### sar 명령어에서 확인되는 `%steal`

게스트에서 `sar -P 0 1` 결과를 보면:

```
%user  %system  %steal  %idle
 50      0        50      0
```

* `%user`: 실제 VCPU에서 게스트 프로세스가 돌아간 시간
* `%steal`: **게스트가 실행되고 싶었지만, PCPU가 다른 작업 중이어서 실행되지 못한 시간**
* 물리 장비 top

```bash
qemu-system-x86  → 50%
inf-loop.py      → 50%
```

* 가상 머신 내부 sar

```bash
CPU %user %system %steal
0   50     0       50
```

✅ `%steal = 50%`: \*\*게스트 입장에선 자신이 실행되지 않은 50% 동안을 '도둑맞았음'\*\*으로 인식

* 가상 머신 내부 top

```bash
inf-loop.py → 80~100%
```

❗ **top은 `%steal`을 감안하지 않음** → 실제보다 높게 나옴

### 4) %steal 필드란? 발생하는 현상: VCPU가 밀려난다 → `%steal` 증가

| 필드       | 의미                                                          |
| -------- | ----------------------------------------------------------- |
| `%user`  | 게스트가 CPU를 직접 사용한 시간                                         |
| `%steal` | 가상 머신(VCPU)이 사용하지 못하고 대기한 시간 (호스트에서 다른 프로세스가 CPU를 사용 중인 시간) |

✅ `steal` 값이 높으면 호스트가 **너무 바쁘거나 다른 VM이 경쟁** 중이라는 뜻입니다.

#### 동작 흐름

| 시간 | PCPU0에서 실행된 스레드  |
| -- | ---------------- |
| T0 | VCPU0 스레드        |
| T1 | 호스트의 inf-loop.py |
| T2 | 다시 VCPU0 스레드     |
| T3 | inf-loop.py      |

* 위처럼 **PCPU0 자원을 두 개가 경쟁**하게 되면,\
  VCPU0은 어떤 시간 동안 **CPU를 사용하지 못하게 됨** → 이 시간은 **게스트 입장에선 도둑맞은 시간** = **steal time**

### 5) 실험용 도구 요약

#### `sched-virt.py`

* 사용자가 엔터를 눌러 부하 시작 타이밍을 제어할 수 있음
* CPU 자원 소모 시각화 그래프 자동 생성

#### `plot_sched_virt.py`

* matplotlib 기반 결과 시각화

### 6) 이게 의미하는 성능 영향

1. **게스트 OS는 CPU가 느려졌다고 판단**
   * `top`에서는 여전히 `99%`로 나와 보이지만, 실제 시간 단위로 보면 밀려난 시간만큼 일 처리가 늦어짐
2. **게스트 성능 저하**
   * `sched.py`, `build`, `DB 쿼리`, `웹 서버 응답 지연` 등 **실제 성능이 떨어진다.**
3. **게스트 sar에서 `%steal` 증가**
   * 시스템이 과부하 상태이거나, **다른 가상 머신이 같은 PCPU에서 돌고 있을 때**도 같은 현상 발생

### ✅ 전체 요약

> **PCPU0에서 VCPU0 스레드 이외의 처리가 동작하면, VCPU0이 밀리게 되고 이는 게스트 OS에서 `%steal`로 보이며 성능 저하가 발생합니다.**

| 항목            | 내용                                           |
| ------------- | -------------------------------------------- |
| VCPU와 PCPU 관계 | VCPU는 qemu 프로세스의 스레드이며, PCPU에서 실행됨           |
| `%steal`의 의미  | VCPU가 실행되지 못한 시간 비율                          |
| 성능 측정 팁       | `sar`, `top` 모두 **호스트와 게스트 양쪽에서 확인**해야 정확함   |
| 성능 차이 원인      | 호스트의 다른 프로세스(예: inf-loop.py)가 VCPU 실행을 방해 가능 |



따라서 성능을 보장해야 하는 VM에서는 다음과 같은 설정이 필요합니다:

* `VCPU`당 전용 `PCPU`를 할당 (pinning)하거나
* 가상 머신의 동시 실행 수를 **PCPU 수 이하**로 제한
* sar에서 `%steal`을 **지속적으로 모니터링**
